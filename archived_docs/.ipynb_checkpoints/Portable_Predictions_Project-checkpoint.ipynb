{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3bbec51-d0ed-4150-8105-18d197488f46",
   "metadata": {},
   "source": [
    "## Portable Predictions: Learning Housing Prices Across Diverse Markets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f21fe39-bd98-4175-9d20-01c7e0171a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Database URL\n",
    "db_url = (\n",
    "    \"postgres://ufnbfacj9c7u80:\"\n",
    "    \"pa129f8c5adad53ef2c90db10cce0c899f8c7bdad022cca4e85a8729b19aad68d\"\n",
    "    \"@ceq2kf3e33g245.cluster-czrs8kj4isg7.us-east-1.rds.amazonaws.com:5432/d9f89h4ju1lleh\"\n",
    ")\n",
    "\n",
    "# Fix dialect\n",
    "db_url = db_url.replace(\"postgres://\", \"postgresql://\")\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = create_engine(db_url)\n",
    "\n",
    "# Optional: Check total rows\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    row_count = connection.execute(text(\"SELECT COUNT(*) FROM acs_pums;\")).scalar()\n",
    "    print(f\"Total rows in acs_pums: {row_count:,}\")\n",
    "\n",
    "# SQL query\n",
    "# - Pull only columns needed\n",
    "# - Clean housing filters\n",
    "# - Ensures required fields are NOT NULL\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT \n",
    "        SERIALNO,\n",
    "        VALP,\n",
    "        TEN,\n",
    "        HINCP,\n",
    "        FINCP,\n",
    "        BDS,\n",
    "        YRBLT,\n",
    "        NP,\n",
    "        PUMA,\n",
    "        REGION,\n",
    "        rt\n",
    "    FROM acs_pums\n",
    "    WHERE\n",
    "        TEN = 1                  -- Owner-occupied only\n",
    "        AND VALP > 0             -- Valid property value\n",
    "        AND HINCP IS NOT NULL\n",
    "        AND FINCP IS NOT NULL\n",
    "        AND BDS IS NOT NULL\n",
    "        AND YRBLT IS NOT NULL\n",
    "        AND NP IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Load to DataFrame\n",
    "print(\"Processing query...\")\n",
    "df_check = pd.read_sql(query, engine)\n",
    "\n",
    "\n",
    "# Quick checks\n",
    "print(\"\\nSample rows:\")\n",
    "print(df_check.head(3))\n",
    "print(\"\\nRows, Columns:\", df_check.shape)\n",
    "print(\"\\nColumns:\", df_check.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebafe6dd-db06-4472-a1b4-dc2d3ea016f6",
   "metadata": {},
   "source": [
    "## Linear Regression (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5604b8b1-8bbc-4667-806c-1211ca5195c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "<<<<<<< local\n",
    "# Data prep\n",
    "df = df_check.copy()\n",
    "\n",
    "# Filter for valid rows\n",
    "df = df[(df[\"ten\"] == 1) & (df[\"valp\"] > 0)]\n",
    "features = [\"hincp\", \"fincp\", \"bds\", \"yrblt\", \"np\"]\n",
    "df = df.dropna(subset=features + [\"valp\"])\n",
    "df[\"valp_log\"] = np.log(df[\"valp\"] + 1)\n",
    "print(f\"Final dataset shape: {df.shape}\")\n",
    "\n",
    "# Split\n",
    "=======\n",
    "# --------------------------\n",
    "# 1) Clean working copy\n",
    "# --------------------------\n",
    "df = df_check.copy()\n",
    "\n",
    "# Example: If your DataFrame has an 'acs_year' column, use that\n",
    "if \"acs_year\" in df.columns:\n",
    "    df[\"house_age\"] = df[\"acs_year\"] - df[\"yrblt\"]\n",
    "else:\n",
    "    # If not, use a known constant year, like 2022\n",
    "    survey_year = 2022\n",
    "    df[\"house_age\"] = survey_year - df[\"yrblt\"]\n",
    "\n",
    "# Income adjustments, if you have ADJINC\n",
    "if \"adjinc\" in df.columns:\n",
    "    df[\"hincp_real\"] = df[\"hincp\"] * df[\"adjinc\"] / 1_000_000\n",
    "    df[\"fincp_real\"] = df[\"fincp\"] * df[\"adjinc\"] / 1_000_000\n",
    "else:\n",
    "    df[\"hincp_real\"] = df[\"hincp\"]\n",
    "    df[\"fincp_real\"] = df[\"fincp\"]\n",
    "\n",
    "# Filter valid rows\n",
    "df = df[(df[\"valp\"] > 0) & (df[\"ten\"] == 1)]\n",
    "\n",
    "features = [\"hincp_real\", \"fincp_real\", \"bds\", \"np\", \"house_age\"]\n",
    "df = df.dropna(subset=features + [\"valp\"])\n",
    "\n",
    "df[\"valp_log\"] = np.log(df[\"valp\"] + 1)\n",
    "\n",
    "print(f\"Final dataset shape: {df.shape}\")\n",
    "print(df[features + [\"valp\", \"valp_log\"]].head())\n",
    "\n",
    "# --------------------------\n",
    "# 2) Split\n",
    "# --------------------------\n",
    ">>>>>>> remote\n",
    "X = df[features]\n",
    "y = df[\"valp_log\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "<<<<<<< local\n",
    "# Train Linear Regression\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "y_pred_log = lr_model.predict(X_test)\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y_test, y_pred_log)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred_log)\n",
    "print(f\"Linear Regression RMSE (log): {rmse:.4f}\")\n",
    "print(f\"Linear Regression R² (log): {r2:.4f}\")\n",
    "coefficients = pd.DataFrame({\"Feature\": features, \"Coefficient\": lr_model.coef_})\n",
    "print(coefficients)\n",
    "\n",
    "# Residuals\n",
    "residuals = y_test - y_pred_log\n",
    "\n",
    "# Subplot grid\n",
    "fig, axs = plt.subplots(3, 2, figsize=(14, 18))\n",
    "\n",
    "# Actual vs Predicted\n",
    "axs[0, 0].scatter(y_test, y_pred_log, alpha=0.4, edgecolor=\"k\")\n",
    "axs[0, 0].plot(\n",
    "    [y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"r--\", linewidth=1\n",
    ")\n",
    "=======\n",
    "# --------------------------\n",
    "# 3) Train\n",
    "# --------------------------\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# --------------------------\n",
    "# 4) Evaluate\n",
    "# --------------------------\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"RMSE (log): {rmse:.4f}\")\n",
    "print(f\"R² (log): {r2:.4f}\")\n",
    "\n",
    "coef = pd.DataFrame({\"Feature\": features, \"Coefficient\": lr.coef_}).sort_values(\n",
    "    by=\"Coefficient\", key=abs, ascending=False\n",
    ")\n",
    "\n",
    "print(\"\\nModel Coefficients:\")\n",
    "print(coef)\n",
    "\n",
    "# --------------------------\n",
    "# 5) Visuals & Save\n",
    "# --------------------------\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "fig, axs = plt.subplots(3, 2, figsize=(14, 18))\n",
    "\n",
    "axs[0, 0].scatter(y_test, y_pred, alpha=0.5, edgecolor=\"k\")\n",
    "axs[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"r--\")\n",
    ">>>>>>> remote\n",
    "axs[0, 0].set_title(\"Actual vs Predicted (Log)\")\n",
    "axs[0, 0].set_xlabel(\"Actual log(Property Value)\")\n",
    "axs[0, 0].set_ylabel(\"Predicted log(Property Value)\")\n",
    "axs[0, 0].grid(True)\n",
    "\n",
    "<<<<<<< local\n",
    "# Residuals\n",
    "axs[0, 1].scatter(y_pred_log, residuals, alpha=0.4, edgecolor=\"k\")\n",
    "axs[0, 1].axhline(0, color=\"red\", linestyle=\"--\")\n",
    "axs[0, 1].set_title(\"Residuals Plot (Log)\")\n",
    "axs[0, 1].set_xlabel(\"Predicted log(Property Value)\")\n",
    "axs[0, 1].set_ylabel(\"Residual (Actual - Predicted)\")\n",
    "axs[0, 1].grid(True)\n",
    "\n",
    "# Histogram of Residuals\n",
    "axs[1, 0].hist(residuals, bins=60, edgecolor=\"k\", color=\"steelblue\")\n",
    "axs[1, 0].set_title(\"Histogram of Residuals (Log)\")\n",
    "=======\n",
    "axs[0, 1].scatter(y_pred, residuals, alpha=0.5, edgecolor=\"k\")\n",
    "axs[0, 1].axhline(0, color=\"red\", linestyle=\"--\")\n",
    "axs[0, 1].set_title(\"Residuals Plot\")\n",
    "axs[0, 1].set_xlabel(\"Predicted log(Property Value)\")\n",
    "axs[0, 1].set_ylabel(\"Residual\")\n",
    "axs[0, 1].grid(True)\n",
    "\n",
    "axs[1, 0].hist(residuals, bins=50, edgecolor=\"k\", color=\"steelblue\")\n",
    "axs[1, 0].set_title(\"Histogram of Residuals\")\n",
    ">>>>>>> remote\n",
    "axs[1, 0].set_xlabel(\"Residual\")\n",
    "axs[1, 0].set_ylabel(\"Frequency\")\n",
    "axs[1, 0].grid(True)\n",
    "\n",
    "<<<<<<< local\n",
    "# Coefficient Bar Plot\n",
    "coefficients.plot(\n",
    "    kind=\"bar\",\n",
    "    x=\"Feature\",\n",
    "    y=\"Coefficient\",\n",
    "    legend=False,\n",
    "    ax=axs[1, 1],\n",
    "    color=\"skyblue\",\n",
    ")\n",
    "axs[1, 1].set_title(\"Regression Coefficients (Log)\")\n",
    "axs[1, 1].axhline(0, color=\"black\", linewidth=0.8)\n",
    "axs[1, 1].set_xlabel(\"Feature\")\n",
    "axs[1, 1].set_ylabel(\"Coefficient\")\n",
    "axs[1, 1].grid(axis=\"y\")\n",
    "\n",
    "# QQ Plot\n",
    "stats.probplot(residuals, dist=\"norm\", plot=axs[2, 0])\n",
    "axs[2, 0].set_title(\"QQ Plot of Residuals (Log)\")\n",
    "axs[2, 0].grid(True)\n",
    "\n",
    "# Empty slot for summary\n",
    "=======\n",
    "coef.plot(\n",
    "    kind=\"bar\",\n",
    "    x=\"Feature\",\n",
    "    y=\"Coefficient\",\n",
    "    ax=axs[1, 1],\n",
    "    legend=False,\n",
    "    color=\"skyblue\",\n",
    ")\n",
    "axs[1, 1].axhline(0, color=\"black\", lw=0.8)\n",
    "axs[1, 1].set_title(\"Regression Coefficients\")\n",
    "\n",
    "stats.probplot(residuals, dist=\"norm\", plot=axs[2, 0])\n",
    "axs[2, 0].set_title(\"QQ Plot of Residuals\")\n",
    "axs[2, 0].grid(True)\n",
    "\n",
    ">>>>>>> remote\n",
    "axs[2, 1].axis(\"off\")\n",
    "axs[2, 1].text(\n",
    "    0.5,\n",
    "    0.5,\n",
    "<<<<<<< local\n",
    "    \"Linear Regression (Log)\\nVisual Summary\",\n",
    "=======\n",
    "    \"Linear Regression\\nVisual Diagnostics\",\n",
    ">>>>>>> remote\n",
    "    ha=\"center\",\n",
    "    va=\"center\",\n",
    "    fontsize=12,\n",
    ")\n",
    "\n",
    "<<<<<<< local\n",
    "# Layout\n",
    "plt.tight_layout()\n",
    "=======\n",
    "plt.tight_layout()\n",
    "\n",
    "save_path = \"/Users/mahekpatel/Library/CloudStorage/Dropbox-Samp/Mahek Patel/Mac/UNC/Data 780/Group Project/Final-Project-DATA780/Images/Linear_Regression_Final.png\"\n",
    "plt.savefig(save_path)\n",
    "print(f\"Saved figure to: {save_path}\")\n",
    "\n",
    ">>>>>>> remote\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acd2c95-fdd8-43a5-a59d-741ccb9cd296",
   "metadata": {},
   "source": [
    "<<<<<<< local\n",
    "## Comments\n",
    "\n",
    "**Dataset:**  \n",
    "- Full ACS PUMS: 3,304,047 owner-occupied housing records (`TEN = 1`, `VALP > 0`)\n",
    "- Target: `log(VALP)`\n",
    "- Predictors: `HINCP`, `FINCP`, `BDS`, `YRBLT`, `NP`\n",
    "\n",
    "**Model performance:**  \n",
    "- **RMSE (log scale):** 3.9583  \n",
    "- **R² (log scale):** 0.0984\n",
    "\n",
    "**Interpretation:**  \n",
    "- The baseline Linear Regression using the log-transformed property value target explains about 9.8% of the variance in log-price, which is typical for a simple linear model with a small set of structural and household income predictors.\n",
    "- The coefficients indicate that `BDS` (number of bedrooms) is the strongest positive driver, consistent with the expectation that more bedrooms add significant value.  \n",
    "- `HINCP` and `FINCP` contribute minimally due to overlap and multicollinearity — household and family income often co-vary, limiting their unique explanatory power in a linear setup.  \n",
    "- `YRBLT` shows a small positive effect: newer homes tend to have higher values.  \n",
    "- `NP` (household size) is negative, suggesting larger household size is associated with slightly lower value when controlling for income and structure.\n",
    "\n",
    "**Diagnostics:**  \n",
    "- The scatter plot of Actual vs. Predicted log-values shows a clear upward trend with visible spread — the diagonal pattern confirms the model picks up real signal, but the spread shows limitations in capturing non-linear patterns.\n",
    "- The residuals plot highlights a distinct funnel shape, showing **heteroskedasticity** — residuals widen for higher predicted log-values, indicating that prediction errors are not constant across the value range.\n",
    "- The histogram of residuals shows a multi-peak pattern, which aligns with real-world housing sub-markets: e.g., low-end, mid-tier, and luxury clusters.\n",
    "- The perfect fit line in the scatter plot makes under- and over-predictions visually clear.\n",
    "- The coefficient bar chart confirms `BDS` dominates positive influence, while `NP` is the main negative.\n",
    "- The QQ plot for residuals confirms that errors deviate from perfect normality — another sign that real-world housing data is not fully linear.\n",
    "\n",
    "**Takeaway:**  \n",
    "This baseline confirms the model captures meaningful trends but leaves significant unexplained variance due to its linear form. These diagnostics demonstrate why adding regularization (Ridge) and flexible non-linear models (Random Forest, XGBoost) are essential next steps to handle heteroskedasticity, non-linear interactions, and structural market clusters.\n",
    "=======\n",
    "## Linear Regression Draft — Updated Baseline\n",
    "\n",
    "This section runs a cleaned linear regression using the updated PUMS housing data.  \n",
    "The setup adjusts household and family income if the `ADJINC` field is present and calculates the `house_age` dynamically using `acs_year` if available — otherwise defaults to a constant survey year fallback.  \n",
    "\n",
    "**Key steps:**\n",
    "- Filters to valid owner-occupied units with non-missing predictors\n",
    "- Computes `house_age` as `survey_year - yrblt` or `acs_year - yrblt`\n",
    "- Applies a log transform to `VALP` for stability\n",
    "- Trains the model, outputs RMSE and R²\n",
    "- Shows coefficients sorted by absolute effect\n",
    "- Plots:\n",
    "  - Actual vs Predicted (Log)\n",
    "  - Residuals vs Predictions\n",
    "  - Histogram of Residuals\n",
    "  - Regression Coefficients\n",
    "  - QQ plot of residuals\n",
    "- Saves the visual summary to your project Dropbox folder for easy inclusion in slides or the report.\n",
    "\n",
    "**Next improvements:**  \n",
    "- Add new predictors (`ACR`, `PUMA`, `REGION`) when their quality is verified\n",
    "- Tune models with Ridge, Random Forest, XGBoost for comparison\n",
    "- Join to ZIP or external features once the geocodes are cleaned up\n",
    "- Add feature standardization or transformation as needed.\n",
    ">>>>>>> remote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b12fbf-c8ff-4194-b8d3-5cb2a6a03e94",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9190d5f-82e7-4030-a8e5-8d59cb231b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "<<<<<<< local\n",
    "# Data prep — assumes df, features & log target already defined\n",
    "X = df[features]\n",
    "y = df[\"valp_log\"]\n",
    "=======\n",
    "# --------------------------\n",
    "# 1) Data Prep\n",
    "# --------------------------\n",
    "df_ridge = df.copy()  # Safe copy\n",
    "\n",
    "# Double-check: features and log target should be set\n",
    "X = df_ridge[features]\n",
    "y = df_ridge[\"valp_log\"]\n",
    ">>>>>>> remote\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "<<<<<<< local\n",
    "# Train Ridge Regression\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "y_pred_ridge = ridge_model.predict(X_test)\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "rmse_ridge = np.sqrt(mse_ridge)\n",
    "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "print(f\"Ridge Regression RMSE (log): {rmse_ridge:.4f}\")\n",
    "print(f\"Ridge Regression R² (log): {r2_ridge:.4f}\")\n",
    "ridge_coefficients = pd.DataFrame(\n",
    "    {\"Feature\": features, \"Coefficient\": ridge_model.coef_}\n",
    ")\n",
    "print(ridge_coefficients)\n",
    "residuals_ridge = y_test - y_pred_ridge\n",
    "\n",
    "# 2-column subplot grid\n",
    "fig, axs = plt.subplots(3, 2, figsize=(14, 18))\n",
    "\n",
    "# Actual vs Predicted\n",
    "axs[0, 0].scatter(y_test, y_pred_ridge, alpha=0.4, color=\"green\", edgecolor=\"k\")\n",
    "axs[0, 0].plot(\n",
    "    [y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"r--\", linewidth=1\n",
    ")\n",
    "=======\n",
    "# --------------------------\n",
    "# 2) Ridge Model\n",
    "# --------------------------\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train, y_train)\n",
    "y_pred_ridge = ridge.predict(X_test)\n",
    "\n",
    "# --------------------------\n",
    "# 3) Evaluate\n",
    "# --------------------------\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "rmse_ridge = np.sqrt(mse_ridge)\n",
    "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "\n",
    "print(f\"Ridge Regression RMSE (log): {rmse_ridge:.4f}\")\n",
    "print(f\"Ridge Regression R² (log): {r2_ridge:.4f}\")\n",
    "\n",
    "ridge_coef = pd.DataFrame(\n",
    "    {\"Feature\": features, \"Coefficient\": ridge.coef_}\n",
    ").sort_values(by=\"Coefficient\", key=abs, ascending=False)\n",
    "\n",
    "print(\"\\nRidge Coefficients:\")\n",
    "print(ridge_coef)\n",
    "\n",
    "# --------------------------\n",
    "# 4) Diagnostics\n",
    "# --------------------------\n",
    "residuals_ridge = y_test - y_pred_ridge\n",
    "\n",
    "fig, axs = plt.subplots(3, 2, figsize=(14, 18))\n",
    "\n",
    "# Actual vs Predicted\n",
    "axs[0, 0].scatter(y_test, y_pred_ridge, alpha=0.4, edgecolor=\"k\", color=\"green\")\n",
    "axs[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"r--\")\n",
    ">>>>>>> remote\n",
    "axs[0, 0].set_title(\"Ridge: Actual vs Predicted (Log)\")\n",
    "axs[0, 0].set_xlabel(\"Actual log(Property Value)\")\n",
    "axs[0, 0].set_ylabel(\"Predicted log(Property Value)\")\n",
    "axs[0, 0].grid(True)\n",
    "\n",
    "# Residuals Plot\n",
    "axs[0, 1].scatter(\n",
    "<<<<<<< local\n",
    "    y_pred_ridge, residuals_ridge, alpha=0.4, color=\"green\", edgecolor=\"k\"\n",
    ")\n",
    "axs[0, 1].axhline(0, color=\"red\", linestyle=\"--\", linewidth=1)\n",
    "axs[0, 1].set_title(\"Residuals Plot (Log)\")\n",
    "axs[0, 1].set_xlabel(\"Predicted log(Property Value)\")\n",
    "axs[0, 1].set_ylabel(\"Residual (Actual - Predicted)\")\n",
    "axs[0, 1].grid(True)\n",
    "\n",
    "# Histogram of Residuals\n",
    "axs[1, 0].hist(residuals_ridge, bins=60, edgecolor=\"k\", color=\"seagreen\")\n",
    "axs[1, 0].set_title(\"Histogram of Residuals (Log)\")\n",
    "=======\n",
    "    y_pred_ridge, residuals_ridge, alpha=0.4, edgecolor=\"k\", color=\"green\"\n",
    ")\n",
    "axs[0, 1].axhline(0, color=\"red\", linestyle=\"--\")\n",
    "axs[0, 1].set_title(\"Ridge Residuals Plot\")\n",
    "axs[0, 1].set_xlabel(\"Predicted log(Property Value)\")\n",
    "axs[0, 1].set_ylabel(\"Residual\")\n",
    "axs[0, 1].grid(True)\n",
    "\n",
    "# Histogram of Residuals\n",
    "axs[1, 0].hist(residuals_ridge, bins=50, edgecolor=\"k\", color=\"seagreen\")\n",
    "axs[1, 0].set_title(\"Histogram of Ridge Residuals\")\n",
    ">>>>>>> remote\n",
    "axs[1, 0].set_xlabel(\"Residual\")\n",
    "axs[1, 0].set_ylabel(\"Frequency\")\n",
    "axs[1, 0].grid(True)\n",
    "\n",
    "<<<<<<< local\n",
    "# Coefficient Bar Plot\n",
    "ridge_coefficients.plot(\n",
    "    kind=\"bar\",\n",
    "    x=\"Feature\",\n",
    "    y=\"Coefficient\",\n",
    "    legend=False,\n",
    "    ax=axs[1, 1],\n",
    "    color=\"limegreen\",\n",
    ")\n",
    "axs[1, 1].axhline(0, color=\"black\", linewidth=0.8)\n",
    "axs[1, 1].set_title(\"Ridge Coefficients (Log)\")\n",
    "=======\n",
    "# Coefficient Plot\n",
    "ridge_coef.plot(\n",
    "    kind=\"bar\",\n",
    "    x=\"Feature\",\n",
    "    y=\"Coefficient\",\n",
    "    ax=axs[1, 1],\n",
    "    legend=False,\n",
    "    color=\"limegreen\",\n",
    ")\n",
    "axs[1, 1].axhline(0, color=\"black\", lw=0.8)\n",
    "axs[1, 1].set_title(\"Ridge Regression Coefficients\")\n",
    ">>>>>>> remote\n",
    "axs[1, 1].set_xlabel(\"Feature\")\n",
    "axs[1, 1].set_ylabel(\"Coefficient\")\n",
    "axs[1, 1].grid(axis=\"y\")\n",
    "\n",
    "<<<<<<< local\n",
    "# QQ Plot for Residuals\n",
    "stats.probplot(residuals_ridge, dist=\"norm\", plot=axs[2, 0])\n",
    "axs[2, 0].set_title(\"QQ Plot of Residuals (Log)\")\n",
    "axs[2, 0].grid(True)\n",
    "\n",
    "# Final Slot for Summary\n",
    "=======\n",
    "# QQ Plot\n",
    "stats.probplot(residuals_ridge, dist=\"norm\", plot=axs[2, 0])\n",
    "axs[2, 0].set_title(\"QQ Plot of Ridge Residuals\")\n",
    "axs[2, 0].grid(True)\n",
    "\n",
    "# Empty slot\n",
    ">>>>>>> remote\n",
    "axs[2, 1].axis(\"off\")\n",
    "axs[2, 1].text(\n",
    "    0.5,\n",
    "    0.5,\n",
    "<<<<<<< local\n",
    "    \"Ridge Regression (Log)\\nVisual Summary\",\n",
    "=======\n",
    "    \"Ridge Regression\\nVisual Diagnostics\",\n",
    ">>>>>>> remote\n",
    "    ha=\"center\",\n",
    "    va=\"center\",\n",
    "    fontsize=12,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "<<<<<<< local\n",
    "=======\n",
    "\n",
    "# Save figure\n",
    "save_path_ridge = \"/Users/mahekpatel/Library/CloudStorage/Dropbox-Samp/Mahek Patel/Mac/UNC/Data 780/Group Project/Final-Project-DATA780/Images/ridge_regression_final.png\"\n",
    "plt.savefig(save_path_ridge)\n",
    "print(f\"Figure saved to: {save_path_ridge}\")\n",
    "\n",
    ">>>>>>> remote\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5874837-4258-4054-a400-3bdbff0307fa",
   "metadata": {},
   "source": [
    "<<<<<<< local\n",
    "### Comments\n",
    "\n",
    "The **Ridge Regression log-transformed results** build on the baseline linear model by adding **L2 regularization**, which shrinks large coefficients to help manage multicollinearity.\n",
    "\n",
    "**Performance:**  \n",
    "The model’s RMSE on the log scale is about **3.96**, with an R² around **0.098**, nearly identical to the plain Linear Regression. This indicates that adding regularization alone does not significantly improve predictive accuracy for this dataset.\n",
    "\n",
    "**Actual vs Predicted:**  \n",
    "The scatter plot shows that predicted values generally follow the true log values but there is clear spread, especially at higher prices where the model underpredicts.\n",
    "\n",
    "**Residuals:**  \n",
    "The residuals plot shows patterns that suggest non-linear effects remain. The histogram of residuals is roughly centered near zero but shows asymmetry, hinting at skew or variance differences in residuals across value ranges.\n",
    "\n",
    "**QQ Plot:**  \n",
    "The QQ plot reveals that residuals deviate from perfect normality, especially in the tails. This is common for real estate price data, which often includes extreme values.\n",
    "\n",
    "**Coefficients:**  \n",
    "The coefficient bar plot shows that **number of bedrooms** remains the dominant positive predictor, with household income and other predictors contributing smaller effects. The regularization effect shrinks coefficient magnitudes compared to the OLS model.\n",
    "\n",
    "**Key takeaway:**  \n",
    "Ridge Regression slightly stabilizes the model but does not resolve the limitations of linearity for this task. This reinforces the need to test more flexible, non-linear models like **Random Forests** or **XGBoost** to better capture complex housing price behavior.\n",
    "=======\n",
    "# Ridge Regression:\n",
    "\n",
    "## What’s working\n",
    "- Ridge regression runs end-to-end without errors.\n",
    "- Diagnostics produce: \n",
    "  - **Actual vs Predicted**  \n",
    "  - **Residuals Plot**  \n",
    "  - **Histogram of Residuals**  \n",
    "  - **Coefficient Bar Plot**  \n",
    "  - **QQ Plot**\n",
    "- Output image is saved automatically to the project folder.\n",
    "\n",
    "## What’s not meaningful yet\n",
    "- The **residual plots** show multi-modal bands and clear outliers.\n",
    "- Coefficient magnitudes are small or inconsistent (income impact is nearly zero).\n",
    "- QQ plot indicates non-normal residuals → likely unmodeled effects.\n",
    "- The histogram confirms skewness and heavy tails.\n",
    "\n",
    "**Root causes:**  \n",
    "- Important predictors are missing (location effect, lot size, quality).\n",
    "- Variables may not be standardized or adjusted for real dollar values.\n",
    "- Possible non-linearity or interaction effects not captured.\n",
    "\n",
    "## What to fix next\n",
    "1. **Filter extreme outliers**: Remove or Winsorize very high or low income or value rows.\n",
    "2. **Adjust income**: Use `adjinc` to normalize household and family income.\n",
    "3. **Add location variables**: Join clean PUMA / ZIP crosswalk and region codes.\n",
    "4. **Create polynomial terms**: Try `house_age²`, `income × bedrooms` to test interactions.\n",
    "5. **Add neighborhood features**: If available, merge crime, school quality, or other proxies.\n",
    "6. **Test log-log transformations**: Consider logging predictors if they are heavily skewed.\n",
    "7. **Stratify your train-test split**: To avoid region-level biases.\n",
    "\n",
    "## Summary\n",
    "- This version is your **baseline Ridge model**.  \n",
    "- Keep the code, figure, and output saved.\n",
    "- The next version should focus on **feature engineering**, **filtering**, and **richer data** for practical prediction quality.\n",
    ">>>>>>> remote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c6b136-44dc-425d-ac65-6a6198476785",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0e7ba2-5a9c-4e45-a4d8-05a08970f484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "<<<<<<< local\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Data prep — assumes df, features & valp_log already defined\n",
    "=======\n",
    "# --------------------------\n",
    "# 1) Start timer\n",
    "# --------------------------\n",
    "start_time = time.time()\n",
    "\n",
    "# --------------------------\n",
    "# 2) Data prep — assumes df, features, valp_log already defined\n",
    "# --------------------------\n",
    ">>>>>>> remote\n",
    "X = df[features]\n",
    "y = df[\"valp_log\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "<<<<<<< local\n",
    "# Train Random Forest\n",
    "=======\n",
    "# --------------------------\n",
    "# 3) Train Random Forest\n",
    "# --------------------------\n",
    ">>>>>>> remote\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100, max_depth=None, random_state=42, n_jobs=-1\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "<<<<<<< local\n",
    "\n",
    "# Evaluate\n",
    "=======\n",
    "# --------------------------\n",
    "# 4) Evaluate\n",
    "# --------------------------\n",
    ">>>>>>> remote\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "rmse_rf = np.sqrt(mse_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"Random Forest RMSE (log): {rmse_rf:.4f}\")\n",
    "print(f\"Random Forest R² (log): {r2_rf:.4f}\")\n",
    "\n",
    "<<<<<<< local\n",
    "# Feature importances\n",
    "=======\n",
    ">>>>>>> remote\n",
    "importances_rf = pd.DataFrame(\n",
    "    {\"Feature\": features, \"Importance\": rf_model.feature_importances_}\n",
    ").sort_values(by=\"Importance\", ascending=False)\n",
    "print(\"\\nFeature Importances:\")\n",
    "print(importances_rf)\n",
    "\n",
    "<<<<<<< local\n",
    "# End timer\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n⏱️ Elapsed time: {elapsed:.2f} seconds\")\n",
    "\n",
    "# Residuals\n",
    "residuals_rf = y_test - y_pred_rf\n",
    "\n",
    "# 2-column subplot grid\n",
    "=======\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nElapsed time: {elapsed:.2f} seconds\")\n",
    "\n",
    "# --------------------------\n",
    "# 5) Diagnostics\n",
    "# --------------------------\n",
    "residuals_rf = y_test - y_pred_rf\n",
    "\n",
    ">>>>>>> remote\n",
    "fig, axs = plt.subplots(3, 2, figsize=(14, 18))\n",
    "\n",
    "# Actual vs Predicted\n",
    "axs[0, 0].scatter(y_test, y_pred_rf, alpha=0.3, color=\"red\", edgecolor=\"k\")\n",
    "axs[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"k--\")\n",
    "axs[0, 0].set_title(\"Random Forest: Actual vs Predicted (Log)\")\n",
    "axs[0, 0].set_xlabel(\"Actual log(Property Value)\")\n",
    "axs[0, 0].set_ylabel(\"Predicted log(Property Value)\")\n",
    "axs[0, 0].grid(True)\n",
    "\n",
    "# Residuals Plot\n",
    "axs[0, 1].scatter(y_pred_rf, residuals_rf, alpha=0.3, color=\"red\", edgecolor=\"k\")\n",
    "axs[0, 1].axhline(y=0, color=\"black\", linestyle=\"--\")\n",
    "<<<<<<< local\n",
    "axs[0, 1].set_title(\"Residuals Plot: Random Forest (Log)\")\n",
    "axs[0, 1].set_xlabel(\"Predicted log(Property Value)\")\n",
    "axs[0, 1].set_ylabel(\"Residual (Actual - Predicted)\")\n",
    "=======\n",
    "axs[0, 1].set_title(\"Random Forest Residuals (Log)\")\n",
    "axs[0, 1].set_xlabel(\"Predicted log(Property Value)\")\n",
    "axs[0, 1].set_ylabel(\"Residual\")\n",
    ">>>>>>> remote\n",
    "axs[0, 1].grid(True)\n",
    "\n",
    "# Histogram of Residuals\n",
    "axs[1, 0].hist(residuals_rf, bins=60, edgecolor=\"k\", color=\"orangered\")\n",
    "<<<<<<< local\n",
    "axs[1, 0].set_title(\"Histogram of Residuals (Log)\")\n",
    "=======\n",
    "axs[1, 0].set_title(\"Histogram of Residuals\")\n",
    ">>>>>>> remote\n",
    "axs[1, 0].set_xlabel(\"Residual\")\n",
    "axs[1, 0].set_ylabel(\"Frequency\")\n",
    "axs[1, 0].grid(True)\n",
    "\n",
    "<<<<<<< local\n",
    "# Feature Importance Bar Plot\n",
    "=======\n",
    "# Feature Importances\n",
    ">>>>>>> remote\n",
    "importances_rf.plot(\n",
    "    kind=\"bar\",\n",
    "    x=\"Feature\",\n",
    "    y=\"Importance\",\n",
    "<<<<<<< local\n",
    "    legend=False,\n",
    "    ax=axs[1, 1],\n",
    "    color=\"firebrick\",\n",
    ")\n",
    "axs[1, 1].set_title(\"Random Forest Feature Importances (Log)\")\n",
    "=======\n",
    "    ax=axs[1, 1],\n",
    "    legend=False,\n",
    "    color=\"firebrick\",\n",
    ")\n",
    "axs[1, 1].set_title(\"Random Forest Feature Importances\")\n",
    "axs[1, 1].axhline(0, color=\"black\", lw=0.8)\n",
    ">>>>>>> remote\n",
    "axs[1, 1].set_xlabel(\"Feature\")\n",
    "axs[1, 1].set_ylabel(\"Importance\")\n",
    "axs[1, 1].grid(axis=\"y\")\n",
    "\n",
    "# QQ Plot\n",
    "stats.probplot(residuals_rf, dist=\"norm\", plot=axs[2, 0])\n",
    "<<<<<<< local\n",
    "axs[2, 0].set_title(\"QQ Plot of Residuals (Log)\")\n",
    "axs[2, 0].grid(True)\n",
    "\n",
    "# Final slot for summary\n",
    "axs[2, 1].axis(\"off\")\n",
    "axs[2, 1].text(\n",
    "    0.5,\n",
    "    0.5,\n",
    "    \"Random Forest (Log)\\nVisual Summary\",\n",
    "    ha=\"center\",\n",
    "    va=\"center\",\n",
    "    fontsize=12,\n",
    ")\n",
    "\n",
    "# Final layout\n",
    "plt.tight_layout()\n",
    "=======\n",
    "axs[2, 0].set_title(\"QQ Plot of Residuals\")\n",
    "axs[2, 0].grid(True)\n",
    "\n",
    "# Empty slot\n",
    "axs[2, 1].axis(\"off\")\n",
    "axs[2, 1].text(\n",
    "    0.5, 0.5, \"Random Forest Diagnostics\", ha=\"center\", va=\"center\", fontsize=12\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# --------------------------\n",
    "# 6) Save to Dropbox path\n",
    "# --------------------------\n",
    "save_path = (\n",
    "    \"/Users/mahekpatel/Library/CloudStorage/Dropbox-Samp/\"\n",
    "    \"Mahek Patel/Mac/UNC/Data 780/Group Project/Final-Project-DATA780/Images/\"\n",
    "    \"random_forest_final.png\"\n",
    ")\n",
    "plt.savefig(save_path)\n",
    "print(f\"Saved Random Forest diagnostics to: {save_path}\")\n",
    "\n",
    ">>>>>>> remote\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3641f65f-64ac-4ffb-85f4-2456d1ceb4d1",
   "metadata": {},
   "source": [
    "<<<<<<< local\n",
    "## Comments\n",
    "\n",
    "- **RMSE:** ~3.22  \n",
    "- **R²:** 0.4040  \n",
    "- **Elapsed Time:** ~88 seconds\n",
    "\n",
    "### Summary\n",
    "\n",
    "The Random Forest model, using a log-transformed property value target, shows a noticeable improvement over linear and ridge models. The actual vs. predicted scatter plot shows tighter alignment along the ideal diagonal, indicating the model is capturing non-linear relationships more effectively.\n",
    "\n",
    "The residuals plot shows tighter clustering around zero compared to the linear baselines, though patterns remain, highlighting that variance is not fully captured at the extremes. The histogram of residuals is more centered and less skewed than previous models, supporting the improved fit.\n",
    "\n",
    "Feature importances show that **household income (`hincp`)**, **family income (`fincp`)**, and **year built (`yrblt`)** continue to be the dominant predictors, confirming consistent variable relevance across methods.\n",
    "\n",
    "Finally, the QQ plot indicates residuals still deviate from perfect normality, which is typical for ensemble tree models.\n",
    "\n",
    "### Takeaway\n",
    "\n",
    "The Random Forest delivers a clear performance gain, handling complex, non-linear patterns better than the simpler models. This suggests tree-based models are more appropriate for this housing price prediction task, especially when combined with the log transformation to stabilize variance.\n",
    "=======\n",
    "## Random Forest Regression — Draft Review\n",
    "\n",
    "### Actual vs Predicted\n",
    "\n",
    "- This plot shows how well your `y_pred_rf` matches the actual `y_test` (log of property value).\n",
    "- The black dashed line is the ideal **perfect prediction** line.\n",
    "- You can see spread around the line — the model is capturing some signal but with clear **variance** and clusters.\n",
    "\n",
    "**Key takeaway:**  \n",
    "Random Forests pick up **nonlinear patterns**, but your predictors don’t fully explain housing price. More **location** and neighborhood detail will help.\n",
    "\n",
    "---\n",
    "\n",
    "### Residuals Plot\n",
    "\n",
    "- The residuals should ideally be random, scattered around zero.\n",
    "- Here you see lines and bands — this means there is **unexplained structure**.\n",
    "- The funnel shape indicates **heteroscedasticity** — the error variance changes with the level of predicted value.\n",
    "\n",
    "**Key takeaway:**  \n",
    "This suggests missing variables — like location, lot size, or property type — which could help flatten out residual structure.\n",
    "\n",
    "---\n",
    "\n",
    "### Histogram of Residuals\n",
    "\n",
    "- You’d want a single normal bell curve.\n",
    "- Instead, there are multiple peaks — this implies you have **subpopulations** in your data not fully modeled.\n",
    "\n",
    "---\n",
    "\n",
    "### Feature Importances\n",
    "\n",
    "- The Random Forest naturally ranks features by split power.\n",
    "- `hincp_real` and `fincp_real` dominate — income is clearly driving the model.\n",
    "- `house_age` and `bds` have smaller weight, but are still useful.\n",
    "- The lack of location factors shows up here — income alone can’t explain neighborhood value differences.\n",
    "\n",
    "---\n",
    "\n",
    "### QQ Plot\n",
    "\n",
    "- If residuals are normal, they hug the red line.\n",
    "- Your QQ plot shows clear deviations — confirming your residuals are not normal.\n",
    "\n",
    "---\n",
    "\n",
    "## General Takeaways\n",
    "\n",
    "- The Random Forest handles nonlinearity better than a simple Linear Regression — this is clear progress.\n",
    "- But visible residual patterns show the need for **richer features**:\n",
    "  - Add **PUMA**, ZIP, or County.\n",
    "  - Bring in lot size (`ACR`), or housing unit type if you have it.\n",
    "  - Add **external scores** like crime or school quality if possible.\n",
    ">>>>>>> remote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98767825-7b4a-4b3f-9725-b89f71243e8a",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fc976f-410c-4993-a39d-2d058c51acdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "<<<<<<< local\n",
    "# Prep fresh working copy\n",
    "df = df_check.copy()\n",
    "df = df[df[\"ten\"] == 1]\n",
    "df = df[df[\"valp\"] > 0]\n",
    "features = [\"hincp\", \"fincp\", \"bds\", \"yrblt\", \"np\"]\n",
    "\n",
    "=======\n",
    "# ==============================\n",
    "# 1) Prep working copy\n",
    "# ==============================\n",
    "df = df_check.copy()\n",
    "df = df[df[\"ten\"] == 1]\n",
    "df = df[df[\"valp\"] > 0]\n",
    "\n",
    "# Add any real income adjust if needed — here skipped\n",
    "df[\"house_age\"] = 2023 - df[\"yrblt\"]\n",
    "\n",
    "features = [\"hincp\", \"fincp\", \"bds\", \"yrblt\", \"np\"]\n",
    ">>>>>>> remote\n",
    "df = df.dropna(subset=features + [\"valp\"])\n",
    "df[\"valp_log\"] = np.log(df[\"valp\"] + 1)\n",
    "\n",
    "print(\"Dataset shape for XGBoost:\", df.shape)\n",
    "\n",
    "X = df[features]\n",
    "y = df[\"valp_log\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "<<<<<<< local\n",
    "=======\n",
    "# ==============================\n",
    "# 2) Train XGBoost\n",
    "# ==============================\n",
    ">>>>>>> remote\n",
    "start_time = time.time()\n",
    "\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    objective=\"reg:squarederror\",\n",
    "    random_state=42,\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "rmse_xgb = np.sqrt(mse_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "<<<<<<< local\n",
    "print(f\"XGBoost RMSE (log scale): {rmse_xgb:.4f}\")\n",
    "print(f\"XGBoost R² (log scale): {r2_xgb:.4f}\")\n",
    "=======\n",
    "print(f\"XGBoost RMSE (log): {rmse_xgb:.4f}\")\n",
    "print(f\"XGBoost R² (log): {r2_xgb:.4f}\")\n",
    ">>>>>>> remote\n",
    "\n",
    "importances_xgb = pd.DataFrame(\n",
    "    {\"Feature\": features, \"Importance\": xgb_model.feature_importances_}\n",
    ").sort_values(by=\"Importance\", ascending=False)\n",
    "<<<<<<< local\n",
    "\n",
    "=======\n",
    ">>>>>>> remote\n",
    "print(\"\\nFeature Importances:\")\n",
    "print(importances_xgb)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "<<<<<<< local\n",
    "print(f\"⏱️ Elapsed time: {elapsed:.2f} seconds\")\n",
    "\n",
    "# Residuals\n",
    "residuals_xgb = y_test - y_pred_xgb\n",
    "\n",
    "# -- 2x2 Subplots Layout --\n",
    "fig, axs = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Actual vs Predicted\n",
    "sns.scatterplot(ax=axs[0, 0], x=y_test, y=y_pred_xgb, color=\"orange\", alpha=0.5)\n",
    "=======\n",
    "print(f\"Elapsed time: {elapsed:.2f} seconds\")\n",
    "\n",
    "# ==============================\n",
    "# 3) Diagnostics\n",
    "# ==============================\n",
    "residuals_xgb = y_test - y_pred_xgb\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Actual vs Predicted\n",
    "sns.scatterplot(ax=axs[0, 0], x=y_test, y=y_pred_xgb, color=\"darkorange\", alpha=0.5)\n",
    ">>>>>>> remote\n",
    "axs[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"k--\")\n",
    "axs[0, 0].set_xlabel(\"Actual log(Property Value)\")\n",
    "axs[0, 0].set_ylabel(\"Predicted log(Property Value)\")\n",
    "axs[0, 0].set_title(\"XGBoost: Actual vs Predicted (Log)\")\n",
    "\n",
    "<<<<<<< local\n",
    "# Residuals Plot\n",
    "sns.scatterplot(ax=axs[0, 1], x=y_pred_xgb, y=residuals_xgb, color=\"orange\", alpha=0.5)\n",
    "=======\n",
    "# Residuals\n",
    "sns.scatterplot(\n",
    "    ax=axs[0, 1], x=y_pred_xgb, y=residuals_xgb, color=\"darkorange\", alpha=0.5\n",
    ")\n",
    ">>>>>>> remote\n",
    "axs[0, 1].axhline(y=0, color=\"black\", linestyle=\"--\")\n",
    "axs[0, 1].set_xlabel(\"Predicted log(Property Value)\")\n",
    "axs[0, 1].set_ylabel(\"Residual (Actual - Predicted)\")\n",
    "axs[0, 1].set_title(\"Residuals Plot: XGBoost (Log)\")\n",
    "\n",
    "<<<<<<< local\n",
    "# Histogram of Residuals\n",
    "sns.histplot(\n",
    "    ax=axs[1, 0], x=residuals_xgb, bins=50, kde=True, color=\"orange\", edgecolor=\"k\"\n",
    "=======\n",
    "# Histogram\n",
    "sns.histplot(\n",
    "    ax=axs[1, 0], x=residuals_xgb, bins=50, kde=True, color=\"darkorange\", edgecolor=\"k\"\n",
    ">>>>>>> remote\n",
    ")\n",
    "axs[1, 0].set_xlabel(\"Residual\")\n",
    "axs[1, 0].set_ylabel(\"Frequency\")\n",
    "axs[1, 0].set_title(\"Histogram of Residuals (Log)\")\n",
    "\n",
    "<<<<<<< local\n",
    "# Feature Importance\n",
    "axs[1, 1].bar(importances_xgb[\"Feature\"], importances_xgb[\"Importance\"], color=\"orange\")\n",
    "=======\n",
    "# Feature Importances\n",
    "axs[1, 1].bar(\n",
    "    importances_xgb[\"Feature\"], importances_xgb[\"Importance\"], color=\"darkorange\"\n",
    ")\n",
    ">>>>>>> remote\n",
    "axs[1, 1].set_xlabel(\"Feature\")\n",
    "axs[1, 1].set_ylabel(\"Importance\")\n",
    "axs[1, 1].set_title(\"XGBoost Feature Importances (Log)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "<<<<<<< local\n",
    "=======\n",
    "\n",
    "# ==============================\n",
    "# 4) Save to project folder\n",
    "# ==============================\n",
    "save_path = \"/Users/mahekpatel/Library/CloudStorage/Dropbox-Samp/Mahek Patel/Mac/UNC/Data 780/Group Project/Final-Project-DATA780/Images/xgboost_regression_final.png\"\n",
    "plt.savefig(save_path)\n",
    "print(f\"Saved XGBoost diagnostics to: {save_path}\")\n",
    "\n",
    ">>>>>>> remote\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbd236e-86c8-4df3-92fd-105a83ad08bf",
   "metadata": {},
   "source": [
    "<<<<<<< local\n",
    "## Comments\n",
    "\n",
    "The XGBoost model was applied using a log transformation of the property value target to stabilize variance and handle skewness typical of real estate price distributions.\n",
    "\n",
    "- **RMSE (log scale)**: ~3.76  \n",
    "- **R² (log scale)**: ~0.1856  \n",
    "- **Elapsed Time**: ~1.7 seconds\n",
    "\n",
    "### Feature Importances\n",
    "\n",
    "| Rank | Feature                  | Importance |\n",
    "|------|--------------------------|-------------|\n",
    "| 1    | Year Built (`yrblt`)     | ~46% |\n",
    "| 2    | Bedrooms (`bds`)         | ~24% |\n",
    "| 3    | Household Income (`hincp`) | ~21% |\n",
    "| 4    | Number of Persons (`np`) | ~5% |\n",
    "| 5    | Family Income (`fincp`)  | ~4% |\n",
    "\n",
    "The feature importance plot shows that **Year Built** (`yrblt`) is the dominant driver for predicting log-scale home prices. This aligns with expectations that newer properties typically command higher values. **Bedrooms** and **Household Income** add predictive value, while **Family Income** and **Household Size** (`np`) contribute marginally.\n",
    "\n",
    "### Plots Summary\n",
    "\n",
    "- **Actual vs. Predicted**: The scatter plot shows that predictions track the general trend well, though some spread indicates model error at the extremes.\n",
    "- **Residuals Plot**: Residuals cluster around zero, but variance is visible — suggesting some heteroskedasticity, which is common in housing data.\n",
    "- **Residuals Histogram**: The residual distribution is mostly normal with minor skewness, showing that the boosting model is able to reduce bias.\n",
    "- **Feature Importances Bar Chart**: Clear visual confirmation that `yrblt` and `bds` dominate this tree-based model.\n",
    "\n",
    "**Key Takeaway**\n",
    "\n",
    "XGBoost effectively captures non-linear relationships in the housing data that basic linear models miss. It highlights the structural age and size of the property as critical predictors for home value. While model interpretability is less transparent than linear models, the performance trade-off can be worthwhile for price prediction tasks where complex patterns matter.\n",
    "=======\n",
    "## XGBoost Regression — Draft Evaluation\n",
    "\n",
    "### Model Setup\n",
    "- **Algorithm:** XGBoost Regressor (`XGBRegressor`)\n",
    "- **Features Used:**  \n",
    "  - Household Income (`hincp`)\n",
    "  - Family Income (`fincp`)\n",
    "  - Bedrooms (`bds`)\n",
    "  - Year Built (`yrblt`)\n",
    "  - Number of Persons (`np`)\n",
    "\n",
    "- **Target:** Property Value (log-transformed)\n",
    "\n",
    "---\n",
    "\n",
    "### Metrics\n",
    "- **RMSE (log):** *(see output above)*\n",
    "- **R² (log):** *(see output above)*\n",
    "- **Elapsed Time:** Printed at runtime\n",
    "\n",
    "---\n",
    "\n",
    "### Visual Diagnostics\n",
    "- **Top-left:** Actual vs Predicted — overall fit\n",
    "- **Top-right:** Residuals vs Predicted — checks for error patterns\n",
    "- **Bottom-left:** Histogram of Residuals — distribution of prediction errors\n",
    "- **Bottom-right:** Feature Importances — how much each feature contributed to splits\n",
    "\n",
    "---\n",
    "\n",
    "### Observations\n",
    "- XGBoost is picking up **nonlinear effects** and interactions.\n",
    "- `yrblt` and `bds` are strong predictors.\n",
    "- Residual plots show clusters — suggesting unmodeled effects like **location** or **house type**.\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps\n",
    "1. Add **location variables** — PUMA, ZIP, or County.\n",
    "2. Test more XGBoost parameters (tree depth, learning rate).\n",
    "3. Add new features — lot size, square footage, house type.\n",
    "4. Try **SHAP** for detailed interpretability.\n",
    "5. Consider more robust target transformations if needed.\n",
    ">>>>>>> remote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d23367c-1d2c-4db1-b87e-983d8511111a",
   "metadata": {},
   "source": [
    "## SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f53173-b11e-4414-be0c-9582ac3bc323",
   "metadata": {},
   "outputs": [],
   "source": [
    "<<<<<<< local\n",
    "import shap\n",
    "\n",
    "# Create SHAP explainer for trained XGBoost model\n",
    "=======\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "# -------------------------------\n",
    "# SHAP for XGBoost with SAVE\n",
    "# -------------------------------\n",
    "\n",
    "# Paths\n",
    "base_path = \"/Users/mahekpatel/Library/CloudStorage/Dropbox-Samp/Mahek Patel/Mac/UNC/Data 780/Group Project/Final-Project-DATA780/Images\"\n",
    "\n",
    "# Create SHAP explainer\n",
    ">>>>>>> remote\n",
    "explainer = shap.Explainer(xgb_model)\n",
    "\n",
    "# Get SHAP values for X_test\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "<<<<<<< local\n",
    "# 1) Global SHAP summary plot\n",
    "print(\"Global SHAP Summary Plot\")\n",
    "shap.summary_plot(shap_values, X_test, plot_size=(10, 6))\n",
    "\n",
    "# 2) Local SHAP force plot for FIRST prediction\n",
    "print(\"Local SHAP Force Plot for First Observation\")\n",
    "shap.initjs()\n",
    "shap.force_plot(\n",
    "    explainer.expected_value, shap_values[0].values, X_test.iloc[0], matplotlib=True\n",
    ")\n",
    "\n",
    "# 3) Dependence plot for 'hincp' (Household Income)\n",
    "print(\"SHAP Dependence Plot for 'hincp'\")\n",
    "shap.dependence_plot(\"hincp\", shap_values.values, X_test)\n",
    "=======\n",
    "# -------------------------------\n",
    "# 1) Global SHAP Summary Plot\n",
    "# -------------------------------\n",
    "print(\"Saving Global SHAP Summary Plot...\")\n",
    "plt.figure()\n",
    "shap.summary_plot(shap_values, X_test, plot_size=(10, 6), show=False)\n",
    "summary_path = f\"{base_path}/shap_summary_plot.png\"\n",
    "plt.savefig(summary_path, bbox_inches=\"tight\", dpi=300)\n",
    "plt.close()\n",
    "print(f\"Saved: {summary_path}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2) Local SHAP Force Plot\n",
    "# -------------------------------\n",
    "print(\"Saving Local SHAP Force Plot for First Row...\")\n",
    "shap.initjs()\n",
    "force_fig = shap.force_plot(\n",
    "    explainer.expected_value, shap_values[0].values, X_test.iloc[0], matplotlib=True\n",
    ")\n",
    "force_path = f\"{base_path}/shap_force_plot_first.png\"\n",
    "plt.savefig(force_path, bbox_inches=\"tight\", dpi=300)\n",
    "plt.close()\n",
    "print(f\"Saved: {force_path}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3) SHAP Dependence Plot Example\n",
    "# -------------------------------\n",
    "print(\"Saving SHAP Dependence Plot for 'hincp'...\")\n",
    "plt.figure()\n",
    "shap.dependence_plot(\"hincp\", shap_values.values, X_test, show=False)\n",
    "dependence_path = f\"{base_path}/shap_dependence_hincp.png\"\n",
    "plt.savefig(dependence_path, bbox_inches=\"tight\", dpi=300)\n",
    "plt.close()\n",
    "print(f\"Saved: {dependence_path}\")\n",
    "\n",
    "print(\"\\nSHAP visualizations saved to Dropbox folder.\")\n",
    ">>>>>>> remote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa9df51-8fda-4854-9496-0355b7e47621",
   "metadata": {},
   "source": [
    "<<<<<<< local\n",
    "## Comments\n",
    "\n",
    "The SHAP visualizations for our final XGBoost model provide insight into how individual features contribute to property value predictions on the log scale.\n",
    "\n",
    "- **Summary Plot:**  \n",
    "  The summary plot shows the overall distribution of SHAP values for each feature across the test set. The year built (`yrblt`) stands out as the most influential driver, with its SHAP values widely spread, indicating a strong impact on predicted log prices. Household income (`hincp`) and bedrooms (`bds`) follow as significant predictors, with `np` (number of persons) and family income (`fincp`) showing more modest effects.\n",
    "\n",
    "- **Local Force Plot:**  \n",
    "  This force plot breaks down how each feature contributes to the prediction for an individual household. For this instance, newer construction year (`yrblt = 3.0`) and a moderate bedroom count (`bds = 2`) push the prediction higher than average, while higher household income (`hincp = 119,600`) and household size (`np = 6`) slightly offset that effect.\n",
    "\n",
    "- **Dependence Plot:**  \n",
    "  The dependence plot illustrates the relationship between `hincp` (household income) and its SHAP value. Higher income levels generally increase the predicted log property value up to a point, but the coloring shows that `yrblt` (year built) interacts heavily — newer houses (pink/red) see a larger marginal gain for a given income compared to older houses (blue).\n",
    "\n",
    "Taken together, these SHAP visuals demonstrate that **year built, household income, and number of bedrooms** are the top factors shaping XGBoost’s predictions. The plots also highlight important feature interactions — for example, income effects depend partly on building age, reinforcing the value of using explainable ML tools to unpack complex real estate relationships.\n",
    "=======\n",
    "## SHAP XGBoost Interpretability — Draft Commentary & Next Steps\n",
    "\n",
    "### What the SHAP Visuals Show\n",
    "\n",
    "**Global SHAP Summary Plot**\n",
    "\n",
    "- **Observation:**  \n",
    "  The summary shows that `yrblt` (year built), `hincp` (household income), and `bds` (bedrooms) have the biggest impact on predictions.\n",
    "- **Potential issue:**  \n",
    "  The SHAP spread for `hincp` and `fincp` is tight — income may not vary enough or is collinear.\n",
    "- **Improvement:**  \n",
    "  Adjust income for inflation if you aren’t already, add per-person income or income-to-price ratios, and check for outliers or caps.\n",
    "\n",
    "---\n",
    "\n",
    "**Local SHAP Force Plot**\n",
    "\n",
    "- **Observation:**  \n",
    "  The force plot for the first prediction shows how each feature pushes the prediction up or down from the base value.  \n",
    "  `yrblt` dominates.\n",
    "- **Potential issue:**  \n",
    "  Low contribution from `hincp` and `fincp` indicates income signal might be lost.\n",
    "- **Improvement:**  \n",
    "  Use `house_age` instead of raw `yrblt` to capture depreciation. Verify raw values for coding errors.\n",
    "\n",
    "---\n",
    "\n",
    "**SHAP Dependence Plot**\n",
    "\n",
    "- **Observation:**  \n",
    "  The dependence plot for `hincp` shows clusters — suggesting thresholds or repeated income bins.\n",
    "- **Potential issue:**  \n",
    "  Flat regions mean the model sees breaks, not smooth trends.\n",
    "- **Improvement:**  \n",
    "  If income is binned, make that explicit or transform it. Remove anomalies and encode non-linear buckets if needed.\n",
    "\n",
    "---\n",
    "\n",
    "### Data Quality & Modeling Flags\n",
    "\n",
    "- **Missing PUMA:**  \n",
    "  `PUMA = -9` means missing or unknown area. Remove or replace with ZIP/county crosswalk.\n",
    "- **Location:**  \n",
    "  If you have ZIPs or counties, add them. PUMA is useful but only if clean.\n",
    "- **Feature gaps:**  \n",
    "  Add lot size (`ACR`), units in structure, region, division, county dummy variables. More real-estate context is key.\n",
    "- **External:**  \n",
    "  Current crime table is too limited. Use only for pilot or drop for now.\n",
    "\n",
    "---\n",
    "\n",
    "### Diagnostics Recap\n",
    "\n",
    "- **Residuals:**  \n",
    "  Histogram and QQ show non-normal residuals. The scatter shows model bias and gaps — expected if predictors are sparse.\n",
    "- **Feature importance:**  \n",
    "  `yrblt` dominates — so condition and age matter. Income and household size are weaker signals in this pass.\n",
    "\n",
    "---\n",
    "\n",
    "### Next Actions\n",
    "\n",
    "- Filter or clean `PUMA = -9`\n",
    "- Join to ZIP or county crosswalk if possible\n",
    "- Add derived features (house age, income per room, price per sq ft if you get it)\n",
    "- Consider regional dummy variables\n",
    "- Handle outliers in income and value\n",
    "- Keep saving diagnostics for versioning\n",
    "- Plan for hyperparameter tuning later — data/feature fixes come first\n",
    ">>>>>>> remote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ce6581-0e33-4117-b253-1002a82e396c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
