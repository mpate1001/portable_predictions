{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6a857f7-d648-444d-9c70-839ab320ac73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop for 2025...\n",
      "No file found for 2025\n",
      "Loop for 2024...\n",
      "No file found for 2024\n",
      "Loop for 2023...\n",
      "Downloading CSV for 2023.\n",
      "Saved csv as acs_pums_hca_2023.csv\n",
      "Loop for 2022...\n",
      "Downloading CSV for 2022.\n",
      "Saved csv as acs_pums_hca_2022.csv\n",
      "Loop for 2021...\n",
      "Downloading CSV for 2021.\n",
      "Saved csv as acs_pums_hca_2021.csv\n",
      "Loop for 2020...\n",
      "Downloading CSV for 2020.\n",
      "Saved csv as acs_pums_hca_2020.csv\n",
      "Loop for 2019...\n",
      "Downloading CSV for 2019.\n",
      "Saved csv as acs_pums_hca_2019.csv\n",
      "Loop for 2018...\n",
      "Downloading CSV for 2018.\n",
      "Saved csv as acs_pums_hca_2018.csv\n",
      "Loop for 2017...\n",
      "Downloading CSV for 2017.\n",
      "Saved csv as acs_pums_hca_2017.csv\n",
      "Loop for 2016...\n",
      "Downloading CSV for 2016.\n",
      "Saved csv as acs_pums_hca_2016.csv\n",
      "Loop for 2015...\n",
      "Downloading CSV for 2015.\n",
      "Saved csv as acs_pums_hca_2015.csv\n",
      "Loop for 2014...\n",
      "Downloading CSV for 2014.\n",
      "Saved csv as acs_pums_hca_2014.csv\n",
      "Loop for 2013...\n",
      "Downloading CSV for 2013.\n",
      "Saved csv as acs_pums_hca_2013.csv\n",
      "Loop for 2012...\n",
      "Downloading CSV for 2012.\n",
      "Saved csv as acs_pums_hca_2012.csv\n",
      "Loop for 2011...\n",
      "Downloading CSV for 2011.\n",
      "Saved csv as acs_pums_hca_2011.csv\n",
      "Loop for 2010...\n",
      "Downloading CSV for 2010.\n",
      "Saved csv as acs_pums_hca_2010.csv\n",
      "Loop for 2009...\n",
      "Downloading CSV for 2009.\n",
      "Saved csv as acs_pums_hca_2009.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "\n",
    "#make directory to save CSV files\n",
    "os.makedirs(\"acs_pums\", exist_ok=True)\n",
    "\n",
    "#ACS 5-Year PUMS data base URL\n",
    "#only goes back to 2009\n",
    "dir_url = \"https://www2.census.gov/programs-surveys/acs/data/pums/{year}/5-Year/csv_hca.zip\"\n",
    "max_year = 2025\n",
    "min_year = 2008\n",
    "#loop through various years to extract CSV files\n",
    "for year in range(max_year, min_year, -1):\n",
    "    url = dir_url.format(year=year)\n",
    "    print(f\"Loop for {year}...\")\n",
    "\n",
    "    #create exception if file is not found\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "    \n",
    "        #if response works\n",
    "        if response.status_code == 200:\n",
    "            print(f\"Downloading CSV for {year}.\")\n",
    "    \n",
    "            #iterate through directory and grab csv\n",
    "            with ZipFile(BytesIO(response.content)) as zf:\n",
    "                for file in zf.namelist():\n",
    "                    if file.endswith(\".csv\"):\n",
    "                        file_name = f\"acs_pums_hca_{year}.csv\"\n",
    "                        with open(os.path.join(\"acs_pums\", file_name), \"wb\") as f_out:\n",
    "                            f_out.write(zf.read(file))\n",
    "                        print(f\"Saved csv as {file_name}\")\n",
    "        else:\n",
    "            print(f\"No file found for {year}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download file for {year}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22f9242-e079-4c3b-80f3-9396e29cc54e",
   "metadata": {},
   "source": [
    "### The CSV's were quite large and we needed an easier way to work with the data. Here we loop through the CSV's we just saved and copy them to our Postgres server created and deployed on Heroku. Clumn names from the CSV will be used to make a create table statement so an empty table is first created. Then we copy the data into it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cf84f3-9261-4272-9bb9-1da09322ba4d",
   "metadata": {},
   "source": [
    "#### Created table in Postgres before sending my data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7f5cd1-8003-479e-81e5-46a245694140",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Column Definitions\n",
    "\n",
    "\n",
    "RT\tRecord Type: H = housing record\n",
    "SERIALNO\tUnique housing unit identifier\n",
    "DIVISION\tCensus division (1–9)\n",
    "PUMA\tPublic Use Microdata Area (geographic region)\n",
    "REGION\tCensus region (1–4)\n",
    "ST\tState FIPS code\n",
    "ADJHSG\tHousing adjustment factor (for inflation)\n",
    "ADJINC\tIncome adjustment factor (for inflation)\n",
    "WGTP\tHousing weight (use for weighted statistics)\n",
    "NP\tNumber of persons in household\n",
    "TYPE\tType of unit (1 = housing unit, 2 = group quarters)\n",
    "ACR\tLot size (acreage)\n",
    "BDS\tNumber of bedrooms\n",
    "BLD\tUnits in structure (1 = detached, 2 = mobile, etc.)\n",
    "CONP\tCondo fee\n",
    "ELEP\tElectricity payment\n",
    "FULP\tFuel payment\n",
    "GASP\tGas payment\n",
    "MRGP\tFirst mortgage payment\n",
    "RMS\tNumber of rooms\n",
    "RNTP\tMonthly rent\n",
    "TEN\tTenure: 1 = owned, 2 = rented\n",
    "VAL\tProperty value (if owned)\n",
    "VEH\tVehicles available\n",
    "WATP\tWater payment\n",
    "YBL\tYear built (grouped)\n",
    "FINCP\tFamily income\n",
    "GRNTP\tGross rent\n",
    "GRPIP\tGross rent as % of income\n",
    "HINCP\tHousehold income\n",
    "MV\tMarket value estimate\n",
    "'''\n",
    "\n",
    "# uto-generate CREATE TABLE statement from my columns\n",
    "def generate_create_table_sql(df, table_name):\n",
    "    type_map = {\n",
    "        'int64': 'BIGINT',\n",
    "        'Int64': 'BIGINT',\n",
    "        'float64': 'FLOAT',\n",
    "        'object': 'TEXT'\n",
    "    }\n",
    "\n",
    "    columns = []\n",
    "    for col, dtype in df.dtypes.items():\n",
    "        sql_type = type_map.get(str(dtype), 'TEXT')\n",
    "        clean_col = col.strip().replace(\" \", \"_\").replace(\"-\", \"_\").lower()\n",
    "        columns.append(f'\"{clean_col}\" {sql_type}')\n",
    "    \n",
    "    column_defs = \",\\n    \".join(columns)\n",
    "    return f'CREATE TABLE IF NOT EXISTS {table_name} (\\n    {column_defs}\\n);'\n",
    "\n",
    "#Generate and print the SQL\n",
    "table_sql = generate_create_table_sql(combined_df, \"acs_pums\")\n",
    "print(table_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dd2084-6af6-4edd-bfa3-430e2c86b446",
   "metadata": {},
   "source": [
    "#### APPEND CSV'S TO SINGLE DATAFRAME\n",
    "\n",
    "Originally wanted this to be the block where we use df.to_sql, but our data is far too large to have it work without failing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18d4d1ca-d9d8-4ce6-81e1-259ac047da82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting acs_pums_hca_2009.csv into database...\n",
      "Lodaed acs_pums_hca_2009.csv\n",
      "Inserting acs_pums_hca_2010.csv into database...\n",
      "Lodaed acs_pums_hca_2010.csv\n",
      "Inserting acs_pums_hca_2011.csv into database...\n",
      "Lodaed acs_pums_hca_2011.csv\n",
      "Inserting acs_pums_hca_2012.csv into database...\n",
      "Lodaed acs_pums_hca_2012.csv\n",
      "Inserting acs_pums_hca_2013.csv into database...\n",
      "Lodaed acs_pums_hca_2013.csv\n",
      "Inserting acs_pums_hca_2014.csv into database...\n",
      "Lodaed acs_pums_hca_2014.csv\n",
      "Inserting acs_pums_hca_2015.csv into database...\n",
      "Lodaed acs_pums_hca_2015.csv\n",
      "Inserting acs_pums_hca_2016.csv into database...\n",
      "Lodaed acs_pums_hca_2016.csv\n",
      "Inserting acs_pums_hca_2017.csv into database...\n",
      "Lodaed acs_pums_hca_2017.csv\n",
      "Inserting acs_pums_hca_2018.csv into database...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nrdee\\AppData\\Local\\Temp\\ipykernel_32172\\2387572488.py:29: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\nrdee\\AppData\\Local\\Temp\\ipykernel_32172\\2387572488.py:35: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lodaed acs_pums_hca_2018.csv\n",
      "Inserting acs_pums_hca_2019.csv into database...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nrdee\\AppData\\Local\\Temp\\ipykernel_32172\\2387572488.py:29: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\nrdee\\AppData\\Local\\Temp\\ipykernel_32172\\2387572488.py:35: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lodaed acs_pums_hca_2019.csv\n",
      "Inserting acs_pums_hca_2020.csv into database...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nrdee\\AppData\\Local\\Temp\\ipykernel_32172\\2387572488.py:29: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\nrdee\\AppData\\Local\\Temp\\ipykernel_32172\\2387572488.py:35: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lodaed acs_pums_hca_2020.csv\n",
      "Inserting acs_pums_hca_2021.csv into database...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nrdee\\AppData\\Local\\Temp\\ipykernel_32172\\2387572488.py:29: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\nrdee\\AppData\\Local\\Temp\\ipykernel_32172\\2387572488.py:35: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lodaed acs_pums_hca_2021.csv\n",
      "Inserting acs_pums_hca_2022.csv into database...\n",
      "Lodaed acs_pums_hca_2022.csv\n",
      "Inserting acs_pums_hca_2023.csv into database...\n",
      "Lodaed acs_pums_hca_2023.csv\n",
      "  RT       SERIALNO  DIVISION    PUMA  REGION   ST   ADJHSG   ADJINC   WGTP  \\\n",
      "0  H  2005000000005         9  6122.0       4  6.0  1098709  1119794  175.0   \n",
      "1  H  2005000000015         9  1505.0       4  6.0  1098709  1119794   41.0   \n",
      "2  H  2005000000033         9  8101.0       4  6.0  1098709  1119794   17.0   \n",
      "3  H  2005000000034         9  8005.0       4  6.0  1098709  1119794   24.0   \n",
      "4  H  2005000000044         9  2405.0       4  6.0  1098709  1119794    9.0   \n",
      "\n",
      "   NP  ...  VEH   WATP  YBL    FINCP   GRNTP  GRPIP     HINCP   MV  acs_year  \\\n",
      "0   2  ...  2.0    1.0  5.0      NaN  1250.0   14.0  104100.0  3.0      2009   \n",
      "1   3  ...  2.0  550.0  7.0  74000.0     NaN    NaN   74000.0  7.0      2009   \n",
      "2   2  ...  2.0    1.0  4.0      NaN     NaN    NaN  150000.0  2.0      2009   \n",
      "3   3  ...  1.0   80.0  3.0  46800.0  1447.0   37.0   46800.0  3.0      2009   \n",
      "4   1  ...  1.0    1.0  5.0      NaN     NaN    NaN   64000.0  5.0      2009   \n",
      "\n",
      "                acs_file  \n",
      "0  acs_pums_hca_2009.csv  \n",
      "1  acs_pums_hca_2009.csv  \n",
      "2  acs_pums_hca_2009.csv  \n",
      "3  acs_pums_hca_2009.csv  \n",
      "4  acs_pums_hca_2009.csv  \n",
      "\n",
      "[5 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "#Folder with ACS CSVs\n",
    "input_dir = r\"C:\\Users\\nrdee\\Documents\\Data\\Data Science Masters\\Data 780\\acs_pums\"\n",
    "\n",
    "#columns we want to keep\n",
    "keep_cols = [\n",
    "    \"RT\", \"SERIALNO\", \"DIVISION\", \"PUMA\", \"REGION\", \"ST\", \"ADJHSG\", \"ADJINC\",\n",
    "    \"WGTP\", \"NP\", \"TYPE\", \"ACR\", \"BDS\", \"BLD\", \"CONP\", \"ELEP\", \"FULP\", \"GASP\",\n",
    "    \"MRGP\", \"RMS\", \"RNTP\", \"TEN\", \"VAL\", \"VEH\", \"WATP\", \"YBL\", \"FINCP\", \"GRNTP\",\n",
    "    \"GRPIP\", \"HINCP\", \"MV\"\n",
    "]\n",
    "\n",
    "#empty list for my dfs\n",
    "dfs = []\n",
    "\n",
    "#Loop through each CSV\n",
    "for file in os.listdir(input_dir):\n",
    "    if file.endswith(\".csv\") and \"acs_pums_hca\" in file:\n",
    "        file_path = os.path.join(input_dir, file)\n",
    "        year = file.split(\"_\")[-1].replace(\".csv\", \"\")\n",
    "        print(f\"Inserting {file} into database...\")\n",
    "\n",
    "        #Load CSV\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        #Create columns for additional info on our subsets of data\n",
    "        df[\"acs_year\"] = int(year)\n",
    "        df[\"acs_file\"] = file\n",
    "\n",
    "        #render the df down to only the columns we need\n",
    "        trimmed_df = df[[col for col in keep_cols + [\"acs_year\", \"acs_file\"] if col in df.columns]]\n",
    "\n",
    "        #Append df to list\n",
    "        dfs.append(trimmed_df)\n",
    "\n",
    "        #Insert into a table (append if exists)\n",
    "        # df.to_sql(\"acs_pums\", con=engine, if_exists=\"append\", index=False,method='multi',chunksize=10000)\n",
    "\n",
    "        print(f\"Lodaed {file}\")\n",
    "\n",
    "# Combine all into one DataFrame\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Preview\n",
    "print(combined_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd29f585-2a15-4ff9-a342-ee715bd72aae",
   "metadata": {},
   "source": [
    "#### QUICK DATA TYPE CHANGE ON SPECIFIC COLUMNS (HAD TROUBLE FIRST TIME LOADING WITH DATA TYPES ACROSS FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1200fe4b-2627-4bc1-96f5-c65d2b795a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path for our temp CSV that will be copied into our db\n",
    "temp_path = \"temp_acs_pums.csv\"\n",
    "\n",
    "#Define columns that we want as integers\n",
    "float_to_int_cols = [\n",
    "    \"PUMA\", \"ST\", \"WGTP\", \"NP\", \"TYPE\", \"ACR\", \"BDS\", \"BLD\", \"CONP\", \"ELEP\",\n",
    "    \"FULP\", \"GASP\", \"MRGP\", \"RMS\", \"RNTP\", \"TEN\", \"VAL\", \"VEH\", \"WATP\", \"YBL\",\n",
    "    \"FINCP\", \"GRNTP\", \"GRPIP\", \"HINCP\", \"MV\"\n",
    "]\n",
    "\n",
    "#Convert columns to int\n",
    "for col in float_to_int_cols:\n",
    "    if col in combined_df.columns:\n",
    "        combined_df[col] = pd.to_numeric(combined_df[col], errors='coerce').round().astype(\"Int64\")\n",
    "\n",
    "combined_df.to_csv(temp_path, index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b1f3d3-f0df-4355-a847-4c74b6e3862a",
   "metadata": {},
   "source": [
    "#### Iterate through data in chunks. Unable to load data all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff4756e-d0fc-497e-88f2-83125136653c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "\n",
    "#set our chunk size\n",
    "chunk_size = 500000\n",
    "\n",
    "#Split and load each chunk\n",
    "for i, chunk in enumerate(np.array_split(combined_df, len(combined_df) // chunk_size + 1)):\n",
<<<<<<< HEAD
    "    if i = 12 then:\n",
    "        temp_chunk_path = f\"temp_chunk_{i}.csv\"\n",
    "        chunk.to_csv(temp_chunk_path, index=False)\n",
    "    \n",
    "        try:\n",
    "            #New connection per chunk\n",
    "            conn = psycopg2.connect(\n",
    "                dbname=\"d9f89h4ju1lleh\",\n",
    "                user=\"ufnbfacj9c7u80\",\n",
    "                password=\"pa129f8c5adad53ef2c90db10cce0c899f8c7bdad022cca4e85a8729b19aad68d\",\n",
    "                host=\"ceq2kf3e33g245.cluster-czrs8kj4isg7.us-east-1.rds.amazonaws.com\",\n",
    "                port=\"5432\"\n",
    "            )\n",
    "            \n",
    "            cur = conn.cursor()\n",
    "    \n",
    "            with open(temp_chunk_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                cur.copy_expert(\"COPY acs_pums FROM STDIN WITH CSV HEADER\", f)\n",
    "    \n",
    "            conn.commit()\n",
    "            cur.close()\n",
    "            conn.close()\n",
    "            print(f\"Inserted chunk {i}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Chunk {i} failed:\", e)\n",
    "    else:\n",
    "        print(f\"SKIPPED {i}\"\n",
=======
    "    temp_chunk_path = f\"temp_chunk_{i}.csv\"\n",
    "    chunk.to_csv(temp_chunk_path, index=False)\n",
    "\n",
    "    try:\n",
    "        #New connection per chunk\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=\"d9f89h4ju1lleh\",\n",
    "            user=\"ufnbfacj9c7u80\",\n",
    "            password=\"pa129f8c5adad53ef2c90db10cce0c899f8c7bdad022cca4e85a8729b19aad68d\",\n",
    "            host=\"ceq2kf3e33g245.cluster-czrs8kj4isg7.us-east-1.rds.amazonaws.com\",\n",
    "            port=\"5432\"\n",
    "        )\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        with open(temp_chunk_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            cur.copy_expert(\"COPY acs_pums FROM STDIN WITH CSV HEADER\", f)\n",
    "\n",
    "        conn.commit()\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        print(f\"Inserted chunk {i}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Chunk {i} failed:\", e)\n",
>>>>>>> refs/remotes/origin/main
    "\n",
    "    os.remove(temp_chunk_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.12.10"
=======
   "version": "3.12.7"
>>>>>>> refs/remotes/origin/main
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
