{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aed04cde-ac9a-4802-8033-b0d8e3317fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c6a857f7-d648-444d-9c70-839ab320ac73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for 2025 data...\n",
      "No file found for 2025\n",
      "Searching for 2024 data...\n",
      "No file found for 2024\n",
      "Searching for 2023 data...\n",
      "Downloading CSV for 2023.\n",
      "Saved csv as acs_pums_hca_2023.csv\n",
      "Searching for 2022 data...\n",
      "Downloading CSV for 2022.\n",
      "Saved csv as acs_pums_hca_2022.csv\n",
      "Searching for 2021 data...\n",
      "Downloading CSV for 2021.\n",
      "Saved csv as acs_pums_hca_2021.csv\n",
      "Searching for 2020 data...\n",
      "Downloading CSV for 2020.\n",
      "Saved csv as acs_pums_hca_2020.csv\n",
      "Searching for 2019 data...\n",
      "Downloading CSV for 2019.\n",
      "Saved csv as acs_pums_hca_2019.csv\n",
      "Searching for 2018 data...\n",
      "Downloading CSV for 2018.\n",
      "Saved csv as acs_pums_hca_2018.csv\n",
      "Searching for 2017 data...\n",
      "Downloading CSV for 2017.\n",
      "Saved csv as acs_pums_hca_2017.csv\n",
      "Searching for 2016 data...\n",
      "Downloading CSV for 2016.\n",
      "Saved csv as acs_pums_hca_2016.csv\n",
      "Searching for 2015 data...\n",
      "Downloading CSV for 2015.\n",
      "Saved csv as acs_pums_hca_2015.csv\n",
      "Searching for 2014 data...\n",
      "Downloading CSV for 2014.\n",
      "Saved csv as acs_pums_hca_2014.csv\n",
      "Searching for 2013 data...\n",
      "Downloading CSV for 2013.\n",
      "Saved csv as acs_pums_hca_2013.csv\n",
      "Searching for 2012 data...\n",
      "Downloading CSV for 2012.\n",
      "Saved csv as acs_pums_hca_2012.csv\n",
      "Searching for 2011 data...\n",
      "Downloading CSV for 2011.\n",
      "Saved csv as acs_pums_hca_2011.csv\n",
      "Searching for 2010 data...\n",
      "Downloading CSV for 2010.\n",
      "Saved csv as acs_pums_hca_2010.csv\n",
      "Searching for 2009 data...\n",
      "Downloading CSV for 2009.\n",
      "Saved csv as acs_pums_hca_2009.csv\n"
     ]
    }
   ],
   "source": [
    "#make directory to save CSV files\n",
    "os.makedirs(\"acs_pums\", exist_ok=True)\n",
    "\n",
    "#ACS 5-Year PUMS data base URL\n",
    "#only goes back to 2009\n",
    "dir_url = \"https://www2.census.gov/programs-surveys/acs/data/pums/{year}/5-Year/csv_hca.zip\"\n",
    "max_year = 2025\n",
    "min_year = 2008\n",
    "#loop through various years to extract CSV files\n",
    "for year in range(max_year, min_year, -1):\n",
    "    url = dir_url.format(year=year)\n",
    "    print(f\"Searching for {year} data...\")\n",
    "\n",
    "    #create exception if file is not found\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "    \n",
    "        #if response works\n",
    "        if response.status_code == 200:\n",
    "            print(f\"Downloading CSV for {year}.\")\n",
    "    \n",
    "            #iterate through directory and grab csv\n",
    "            with ZipFile(BytesIO(response.content)) as zf:\n",
    "                for file in zf.namelist():\n",
    "                    if file.endswith(\".csv\"):\n",
    "                        file_name = f\"acs_pums_hca_{year}.csv\"\n",
    "                        with open(os.path.join(\"acs_pums\", file_name), \"wb\") as f_out:\n",
    "                            f_out.write(zf.read(file))\n",
    "                        print(f\"Saved csv as {file_name}\")\n",
    "        else:\n",
    "            print(f\"No file found for {year}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download file for {year}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22f9242-e079-4c3b-80f3-9396e29cc54e",
   "metadata": {},
   "source": [
    "### The CSV's were quite large and we needed an easier way to work with the data. Here we loop through the CSV's we just saved and copy them to our Postgres server created and deployed on Heroku. Clumn names from the CSV will be used to make a create table statement so an empty table is first created. Then we copy the data into it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dd2084-6af6-4edd-bfa3-430e2c86b446",
   "metadata": {},
   "source": [
    "#### APPEND CSV'S TO SINGLE DATAFRAME\n",
    "\n",
    "Originally wanted this to be the block where we use df.to_sql, but our data is far too large to have it work without failing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ad7c75c1-88c3-4919-a4ef-2a8440aceb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading columns from acs_pums_hca_2009.csv...\n",
      "Reading columns from acs_pums_hca_2010.csv...\n",
      "Reading columns from acs_pums_hca_2011.csv...\n",
      "Reading columns from acs_pums_hca_2012.csv...\n",
      "Reading columns from acs_pums_hca_2013.csv...\n",
      "Reading columns from acs_pums_hca_2014.csv...\n",
      "Reading columns from acs_pums_hca_2015.csv...\n",
      "Reading columns from acs_pums_hca_2016.csv...\n",
      "Reading columns from acs_pums_hca_2017.csv...\n",
      "Reading columns from acs_pums_hca_2018.csv...\n",
      "Reading columns from acs_pums_hca_2019.csv...\n",
      "Reading columns from acs_pums_hca_2020.csv...\n",
      "Reading columns from acs_pums_hca_2021.csv...\n",
      "Reading columns from acs_pums_hca_2022.csv...\n",
      "Reading columns from acs_pums_hca_2023.csv...\n"
     ]
    }
   ],
   "source": [
    "# Folder with ACS CSVs\n",
    "input_dir = r\"C:\\Users\\nrdee\\Documents\\Data\\Data Science Masters\\Data 780\\acs_pums\"\n",
    "\n",
    "# Dictionary to collect column names per year\n",
    "columns_by_year = {}\n",
    "\n",
    "# Loop through each CSV\n",
    "for file in os.listdir(input_dir):\n",
    "    if file.endswith(\".csv\") and \"acs_pums_hca\" in file:\n",
    "        file_path = os.path.join(input_dir, file)\n",
    "        year = file.split(\"_\")[-1].replace(\".csv\", \"\")\n",
    "\n",
    "        print(f\"Reading columns from {file}...\")\n",
    "\n",
    "        # Read just the header row\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            header_line = f.readline().strip()\n",
    "\n",
    "        # Parse column names\n",
    "        column_names = header_line.split(',')\n",
    "        columns_by_year[year] = column_names\n",
    "\n",
    "# Convert to DataFrame for visualization\n",
    "col_df = pd.DataFrame.from_dict(columns_by_year, orient='index').sort_index()\n",
    "\n",
    "# Preview\n",
    "col_df.head()\n",
    "col_df.to_csv('columns.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9ef5fe-7905-4b14-8cb5-ddeb09e914d7",
   "metadata": {},
   "source": [
    "### Column Reference Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf37290-e8cd-4944-bea6-c7c37d4e6cd9",
   "metadata": {},
   "source": [
    "| Column             | Description                                                                 |\n",
    "|--------------------|-----------------------------------------------------------------------------|\n",
    "| ACR                | Lot size (acreage)                                                          |\n",
    "| ADJHSG             | Housing adjustment factor                                                   |\n",
    "| ADJINC             | Income adjustment factor                                                    |\n",
    "| AGS                | Aggregate gross rent as percentage of income                                |\n",
    "| BDS                | Number of bedrooms                                                          |\n",
    "| BLD                | Building type (units in structure)                                          |\n",
    "| DIVISION           | Census division code                                                        |\n",
    "| FINCP              | Family income                                                              |\n",
    "| HINCP              | Household income                                                           |\n",
    "| HUPAC              | Presence and age of children in household                                   |\n",
    "| NP                 | Number of persons in household                                              |\n",
    "| NR                 | Number of rooms                                                             |\n",
    "| NRC                | Number of rooms for computing occupants per room                            |\n",
    "| PUMA               | Public Use Microdata Area (geographic identifier)                           |\n",
    "| PUMA_NORMALIZED    | Standardized PUMA column across years                                       |\n",
    "| REGION             | Census region code                                                          |\n",
    "| RESMODE            | Residence 1 year ago                                                        |\n",
    "| RMSP               | Monthly rent                                                               |\n",
    "| RT                 | Record type (H = housing record)                                            |\n",
    "| SERIALNO           | Unique housing unit identifier                                              |\n",
    "| ST                 | State FIPS code                                                             |\n",
    "| SVAL               | Subsidized housing value                                                    |\n",
    "| TEN                | Tenure (e.g., owned, rented)                                                |\n",
    "| TYPE               | Type of unit (e.g., housing unit, group quarters)                           |\n",
    "| VALP               | Property value                                                              |\n",
    "| YRBLT              | Year structure was built                                                    |\n",
    "| acs_year           | Year of the ACS dataset (user-added)                                        |\n",
    "| acs_file           | Source file name (user-added)                                               |\n",
    "| BATH               | Number of bathrooms                                                         |\n",
    "| PUMA00             | PUMA code used in older datasets                                            |\n",
    "| PUMA10             | PUMA code used in 2010-era datasets                                         |\n",
    "| PUMA20             | PUMA code used in 2020-era datasets                                         |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18d4d1ca-d9d8-4ce6-81e1-259ac047da82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending acs_pums_hca_2009.csv to df...\n",
      "Lodaed acs_pums_hca_2009.csv\n",
      "Appending acs_pums_hca_2010.csv to df...\n",
      "Lodaed acs_pums_hca_2010.csv\n",
      "Appending acs_pums_hca_2011.csv to df...\n",
      "Lodaed acs_pums_hca_2011.csv\n",
      "Appending acs_pums_hca_2012.csv to df...\n",
      "Lodaed acs_pums_hca_2012.csv\n",
      "Appending acs_pums_hca_2013.csv to df...\n",
      "Lodaed acs_pums_hca_2013.csv\n",
      "Appending acs_pums_hca_2014.csv to df...\n",
      "Lodaed acs_pums_hca_2014.csv\n",
      "Appending acs_pums_hca_2015.csv to df...\n",
      "Lodaed acs_pums_hca_2015.csv\n",
      "Appending acs_pums_hca_2016.csv to df...\n",
      "Lodaed acs_pums_hca_2016.csv\n",
      "Appending acs_pums_hca_2017.csv to df...\n",
      "Lodaed acs_pums_hca_2017.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nrdee\\AppData\\Local\\Temp\\ipykernel_19068\\3418735750.py:35: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending acs_pums_hca_2018.csv to df...\n",
      "Lodaed acs_pums_hca_2018.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nrdee\\AppData\\Local\\Temp\\ipykernel_19068\\3418735750.py:35: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending acs_pums_hca_2019.csv to df...\n",
      "Lodaed acs_pums_hca_2019.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nrdee\\AppData\\Local\\Temp\\ipykernel_19068\\3418735750.py:35: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending acs_pums_hca_2020.csv to df...\n",
      "Lodaed acs_pums_hca_2020.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nrdee\\AppData\\Local\\Temp\\ipykernel_19068\\3418735750.py:35: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending acs_pums_hca_2021.csv to df...\n",
      "Lodaed acs_pums_hca_2021.csv\n",
      "Appending acs_pums_hca_2022.csv to df...\n",
      "Lodaed acs_pums_hca_2022.csv\n",
      "Appending acs_pums_hca_2023.csv to df...\n",
      "Lodaed acs_pums_hca_2023.csv\n"
     ]
    }
   ],
   "source": [
    "#Folder with ACS CSVs\n",
    "input_dir = r\"C:\\Users\\nrdee\\Documents\\Data\\Data Science Masters\\Data 780\\acs_pums\"\n",
    "\n",
    "#columns we want to keep\n",
    "keep_cols = [\n",
    "'ACR','ADJHSG','ADJINC','AGS','BATH','BDS','BDSP','BLD','DIVISION','FINCP','HINCP','HUPAC','NP','NR','NRC','PUMA',\n",
    "'PUMA00','PUMA10','PUMA20','PUMA_NORMALIZED','REGION','RESMODE','RMS','RMSP','RT','SERIALNO','ST','STATE','SVAL','TEN','TYPE','TYPEHUGQ',\n",
    "'VAL','VALP','YBL','YRBLT'\n",
    "]\n",
    "\n",
    "#Column normalization mapping\n",
    "column_standardization = {\n",
    "    \"ST\": \"STATE\",\n",
    "    \"TYPEHUGQ\": \"TYPE\",\n",
    "    \"BDSP\": \"BDS\",\n",
    "    \"RMS\": \"RMSP\",\n",
    "    \"VAL\": \"VALP\",\n",
    "    \"YBL\": \"YRBLT\"\n",
    "}\n",
    "\n",
    "#empty list for my dfs\n",
    "dfs = []\n",
    "\n",
    "#Loop through each CSV\n",
    "for file in os.listdir(input_dir):\n",
    "    if file.endswith(\".csv\") and \"acs_pums_hca\" in file:\n",
    "        file_path = os.path.join(input_dir, file)\n",
    "        year = file.split(\"_\")[-1].replace(\".csv\", \"\")\n",
    "\n",
    "        #Load CSV\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        #Normalize column names\n",
    "        df.columns = [column_standardization.get(col, col) for col in df.columns]\n",
    "\n",
    "        # Normalize PUMA directly\n",
    "        if \"PUMA20\" in df:\n",
    "            df[\"PUMA_NORMALIZED\"] = df[\"PUMA20\"]\n",
    "        elif \"PUMA10\" in df:\n",
    "            df[\"PUMA_NORMALIZED\"] = df[\"PUMA10\"]\n",
    "        elif \"PUMA00\" in df:\n",
    "            df[\"PUMA_NORMALIZED\"] = df[\"PUMA00\"]\n",
    "        elif \"PUMA\" in df:\n",
    "            df[\"PUMA_NORMALIZED\"] = df[\"PUMA\"]\n",
    "        else: \n",
    "            df[\"PUMA_NORMALIZED\"] = pd.NA\n",
    "            \n",
    "        #Create columns for additional info on our subsets of data\n",
    "        df[\"acs_year\"] = int(year)\n",
    "        df[\"acs_file\"] = file\n",
    "\n",
    "        #render the df down to only the columns we need\n",
    "        trimmed_df = df[[col for col in keep_cols + [\"acs_year\", \"acs_file\"] if col in df.columns]]\n",
    "\n",
    "        print(f\"Appending {file} to df...\")\n",
    "        #Append df to list\n",
    "        dfs.append(trimmed_df)\n",
    "\n",
    "        #Insert into a table (append if exists)\n",
    "        # df.to_sql(\"acs_pums\", con=engine, if_exists=\"append\", index=False,method='multi',chunksize=10000)\n",
    "\n",
    "        print(f\"Lodaed {file}\")\n",
    "\n",
    "# Combine all into one DataFrame\n",
    "combined_df = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0e24133-cfea-490a-bd35-0d93dd317607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACR</th>\n",
       "      <th>ADJHSG</th>\n",
       "      <th>ADJINC</th>\n",
       "      <th>AGS</th>\n",
       "      <th>BDS</th>\n",
       "      <th>BLD</th>\n",
       "      <th>DIVISION</th>\n",
       "      <th>FINCP</th>\n",
       "      <th>HINCP</th>\n",
       "      <th>HUPAC</th>\n",
       "      <th>NP</th>\n",
       "      <th>NR</th>\n",
       "      <th>NRC</th>\n",
       "      <th>PUMA</th>\n",
       "      <th>PUMA_NORMALIZED</th>\n",
       "      <th>REGION</th>\n",
       "      <th>RESMODE</th>\n",
       "      <th>RMSP</th>\n",
       "      <th>RT</th>\n",
       "      <th>SERIALNO</th>\n",
       "      <th>STATE</th>\n",
       "      <th>SVAL</th>\n",
       "      <th>TEN</th>\n",
       "      <th>TYPE</th>\n",
       "      <th>VALP</th>\n",
       "      <th>YRBLT</th>\n",
       "      <th>acs_year</th>\n",
       "      <th>acs_file</th>\n",
       "      <th>BATH</th>\n",
       "      <th>PUMA00</th>\n",
       "      <th>PUMA10</th>\n",
       "      <th>PUMA20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1098709</td>\n",
       "      <td>1119794</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104100.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6122.0</td>\n",
       "      <td>6122</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>H</td>\n",
       "      <td>2005000000005</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2009</td>\n",
       "      <td>acs_pums_hca_2009.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1098709</td>\n",
       "      <td>1119794</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9</td>\n",
       "      <td>74000.0</td>\n",
       "      <td>74000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1505.0</td>\n",
       "      <td>1505</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>H</td>\n",
       "      <td>2005000000015</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2009</td>\n",
       "      <td>acs_pums_hca_2009.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1098709</td>\n",
       "      <td>1119794</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8101.0</td>\n",
       "      <td>8101</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>H</td>\n",
       "      <td>2005000000033</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2009</td>\n",
       "      <td>acs_pums_hca_2009.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1098709</td>\n",
       "      <td>1119794</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9</td>\n",
       "      <td>46800.0</td>\n",
       "      <td>46800.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8005.0</td>\n",
       "      <td>8005</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>H</td>\n",
       "      <td>2005000000034</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2009</td>\n",
       "      <td>acs_pums_hca_2009.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1098709</td>\n",
       "      <td>1119794</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64000.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2405.0</td>\n",
       "      <td>2405</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>H</td>\n",
       "      <td>2005000000044</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2009</td>\n",
       "      <td>acs_pums_hca_2009.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ACR   ADJHSG   ADJINC  AGS  BDS  BLD  DIVISION    FINCP     HINCP  HUPAC  \\\n",
       "0  NaN  1098709  1119794  NaN  2.0  8.0         9      NaN  104100.0    4.0   \n",
       "1  1.0  1098709  1119794  NaN  3.0  2.0         9  74000.0   74000.0    1.0   \n",
       "2  NaN  1098709  1119794  NaN  2.0  9.0         9      NaN  150000.0    4.0   \n",
       "3  1.0  1098709  1119794  NaN  3.0  2.0         9  46800.0   46800.0    2.0   \n",
       "4  1.0  1098709  1119794  NaN  2.0  3.0         9      NaN   64000.0    4.0   \n",
       "\n",
       "   NP   NR  NRC    PUMA  PUMA_NORMALIZED  REGION  RESMODE  RMSP RT  \\\n",
       "0   2  1.0  0.0  6122.0             6122       4      2.0   4.0  H   \n",
       "1   3  0.0  1.0  1505.0             1505       4      2.0   5.0  H   \n",
       "2   2  1.0  0.0  8101.0             8101       4      1.0   3.0  H   \n",
       "3   3  0.0  1.0  8005.0             8005       4      1.0   5.0  H   \n",
       "4   1  0.0  0.0  2405.0             2405       4      1.0   5.0  H   \n",
       "\n",
       "        SERIALNO  STATE  SVAL  TEN  TYPE  VALP  YRBLT  acs_year  \\\n",
       "0  2005000000005      6   0.0  3.0     1   NaN    5.0      2009   \n",
       "1  2005000000015      6   1.0  1.0     1  21.0    7.0      2009   \n",
       "2  2005000000033      6   0.0  1.0     1  21.0    4.0      2009   \n",
       "3  2005000000034      6   0.0  3.0     1   NaN    3.0      2009   \n",
       "4  2005000000044      6   1.0  1.0     1  21.0    5.0      2009   \n",
       "\n",
       "                acs_file  BATH  PUMA00  PUMA10  PUMA20  \n",
       "0  acs_pums_hca_2009.csv   NaN     NaN     NaN     NaN  \n",
       "1  acs_pums_hca_2009.csv   NaN     NaN     NaN     NaN  \n",
       "2  acs_pums_hca_2009.csv   NaN     NaN     NaN     NaN  \n",
       "3  acs_pums_hca_2009.csv   NaN     NaN     NaN     NaN  \n",
       "4  acs_pums_hca_2009.csv   NaN     NaN     NaN     NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "effdb816-9314-4f9e-9c62-f01fd2021a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ACR                float64\n",
       "ADJHSG               int64\n",
       "ADJINC               int64\n",
       "AGS                float64\n",
       "BDS                float64\n",
       "BLD                float64\n",
       "DIVISION             int64\n",
       "FINCP              float64\n",
       "HINCP              float64\n",
       "HUPAC              float64\n",
       "NP                   int64\n",
       "NR                 float64\n",
       "NRC                float64\n",
       "PUMA               float64\n",
       "PUMA_NORMALIZED      int64\n",
       "REGION               int64\n",
       "RESMODE            float64\n",
       "RMSP               float64\n",
       "RT                  object\n",
       "SERIALNO            object\n",
       "STATE                int64\n",
       "SVAL               float64\n",
       "TEN                float64\n",
       "TYPE                 int64\n",
       "VALP               float64\n",
       "YRBLT              float64\n",
       "acs_year             int64\n",
       "acs_file            object\n",
       "BATH               float64\n",
       "PUMA00             float64\n",
       "PUMA10             float64\n",
       "PUMA20             float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f0fa32-435e-401a-afd6-7e0eec47bb88",
   "metadata": {},
   "source": [
    "#### Created table in Postgres before sending my data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0ce1ecd-90ea-49b0-88ca-53e51430983a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE IF NOT EXISTS acs_pums (\n",
      "    \"acr\" FLOAT,\n",
      "    \"adjhsg\" INT,\n",
      "    \"adjinc\" INT,\n",
      "    \"ags\" FLOAT,\n",
      "    \"bds\" FLOAT,\n",
      "    \"bld\" FLOAT,\n",
      "    \"division\" INT,\n",
      "    \"fincp\" FLOAT,\n",
      "    \"hincp\" FLOAT,\n",
      "    \"hupac\" FLOAT,\n",
      "    \"np\" INT,\n",
      "    \"nr\" FLOAT,\n",
      "    \"nrc\" FLOAT,\n",
      "    \"puma\" FLOAT,\n",
      "    \"puma_normalized\" INT,\n",
      "    \"region\" INT,\n",
      "    \"resmode\" FLOAT,\n",
      "    \"rmsp\" FLOAT,\n",
      "    \"rt\" TEXT,\n",
      "    \"serialno\" TEXT,\n",
      "    \"state\" INT,\n",
      "    \"sval\" FLOAT,\n",
      "    \"ten\" FLOAT,\n",
      "    \"type\" INT,\n",
      "    \"valp\" FLOAT,\n",
      "    \"yrblt\" FLOAT,\n",
      "    \"acs_year\" INT,\n",
      "    \"acs_file\" TEXT,\n",
      "    \"bath\" FLOAT,\n",
      "    \"puma00\" FLOAT,\n",
      "    \"puma10\" FLOAT,\n",
      "    \"puma20\" FLOAT\n",
      ");\n"
     ]
    }
   ],
   "source": [
    "#Auto-generate CREATE TABLE statement from my columns\n",
    "def generate_create_table_sql(df, table_name):\n",
    "    type_map = {\n",
    "        'int64': 'INT',\n",
    "        'Int64': 'INT',\n",
    "        'float64': 'FLOAT',\n",
    "        'object': 'TEXT'\n",
    "    }\n",
    "\n",
    "    columns = []\n",
    "    for col, dtype in df.dtypes.items():\n",
    "\n",
    "        clean_col = col.strip().replace(\" \", \"_\").replace(\"-\", \"_\").lower()\n",
    "        \n",
    "        #Ensure serial num is text\n",
    "        if clean_col == 'serialno':\n",
    "            sql_type = 'TEXT'\n",
    "        else:\n",
    "            sql_type = type_map.get(str(dtype), 'TEXT')\n",
    "\n",
    "        columns.append(f'\"{clean_col}\" {sql_type}') \n",
    "    \n",
    "    column_defs = \",\\n    \".join(columns)\n",
    "    return f'CREATE TABLE IF NOT EXISTS {table_name} (\\n    {column_defs}\\n);'\n",
    "\n",
    "#Generate and print the SQL\n",
    "table_sql = generate_create_table_sql(combined_df, \"acs_pums\")\n",
    "print(table_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b1f3d3-f0df-4355-a847-4c74b6e3862a",
   "metadata": {},
   "source": [
    "#### Iterate through data in chunks. Unable to load data all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ff4756e-d0fc-497e-88f2-83125136653c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nrdee\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted chunk 0\n",
      "Inserted chunk 1\n",
      "Inserted chunk 2\n",
      "Inserted chunk 3\n",
      "Inserted chunk 4\n",
      "Inserted chunk 5\n",
      "Inserted chunk 6\n",
      "Inserted chunk 7\n",
      "Inserted chunk 8\n",
      "Inserted chunk 9\n",
      "Inserted chunk 10\n",
      "Inserted chunk 11\n",
      "Chunk 12 failed: server closed the connection unexpectedly\n",
      "\tThis probably means the server terminated abnormally\n",
      "\tbefore or while processing the request.\n",
      "server closed the connection unexpectedly\n",
      "\tThis probably means the server terminated abnormally\n",
      "\tbefore or while processing the request.\n",
      "invalid socket\n",
      "\n",
      "Inserted chunk 13\n",
      "Inserted chunk 14\n",
      "Inserted chunk 15\n",
      "Inserted chunk 16\n",
      "Inserted chunk 17\n",
      "Inserted chunk 18\n",
      "Inserted chunk 19\n",
      "Inserted chunk 20\n",
      "Inserted chunk 21\n",
      "Inserted chunk 22\n"
     ]
    }
   ],
   "source": [
    "#Path for our temp CSV that will be copied into our db\n",
    "temp_path = \"temp_acs_pums.csv\"\n",
    "combined_df.to_csv(temp_path, index=False, header=True)\n",
    "\n",
    "#set our chunk size\n",
    "chunk_size = 500000\n",
    "\n",
    "#Split and load each chunk\n",
    "for i, chunk in enumerate(np.array_split(combined_df, len(combined_df) // chunk_size + 1)):\n",
    "    if i = 12 then:\n",
    "        temp_chunk_path = f\"temp_chunk_{i}.csv\"\n",
    "        chunk.to_csv(temp_chunk_path, index=False)\n",
    "    \n",
    "        try:\n",
    "            #New connection per chunk\n",
    "            conn = psycopg2.connect(\n",
    "                dbname=\"d9f89h4ju1lleh\",\n",
    "                user=\"ufnbfacj9c7u80\",\n",
    "                password=\"pa129f8c5adad53ef2c90db10cce0c899f8c7bdad022cca4e85a8729b19aad68d\",\n",
    "                host=\"ceq2kf3e33g245.cluster-czrs8kj4isg7.us-east-1.rds.amazonaws.com\",\n",
    "                port=\"5432\"\n",
    "            )\n",
    "            cur = conn.cursor()\n",
    "    \n",
    "            with open(temp_chunk_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                cur.copy_expert(\"COPY acs_pums FROM STDIN WITH CSV HEADER\", f)\n",
    "    \n",
    "            conn.commit()\n",
    "            cur.close()\n",
    "            conn.close()\n",
    "            print(f\"Inserted chunk {i}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Chunk {i} failed:\", e)\n",
    "    else:\n",
    "        print(f\"SKIPPED {i}\"\n",
    "\n",
    "    os.remove(temp_chunk_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918e71d1-034b-4471-93bd-5706106572d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
